{
  "version": 3,
  "sources": ["../../openai/src/version.ts", "../../openai/src/streaming.ts", "../../openai/src/error.ts", "../../openai/src/_shims/agent.ts", "../../openai/_shims/fetch.mjs", "../../openai/_shims/formdata.mjs", "../../openai/src/_shims/getMultipartRequestOptions.ts", "../../openai/src/_shims/fileFromPath.ts", "../../openai/src/_shims/node-readable.ts", "../../openai/src/uploads.ts", "../../openai/src/core.ts", "../../openai/src/pagination.ts", "../../openai/src/resource.ts", "../../openai/src/resources/audio/transcriptions.ts", "../../openai/src/resources/audio/translations.ts", "../../openai/src/resources/audio/audio.ts", "../../openai/src/resources/chat/completions.ts", "../../openai/src/resources/chat/chat.ts", "../../openai/src/resources/completions.ts", "../../openai/src/resources/embeddings.ts", "../../openai/src/resources/edits.ts", "../../openai/src/resources/files.ts", "../../openai/src/resources/fine-tunes.ts", "../../openai/src/resources/images.ts", "../../openai/src/resources/models.ts", "../../openai/src/resources/moderations.ts", "../../openai/src/index.ts"],
  "sourcesContent": ["export const VERSION = '4.0.1';\n", "import type { Response } from './_shims/fetch.js';\n\ntype Bytes = string | ArrayBuffer | Uint8Array | Buffer | null | undefined;\n\ntype ServerSentEvent = {\n  event: string | null;\n  data: string;\n  raw: string[];\n};\n\nexport class Stream<Item> implements AsyncIterable<Item> {\n  private decoder: SSEDecoder;\n\n  constructor(private response: Response, private controller: AbortController) {\n    this.decoder = new SSEDecoder();\n  }\n\n  private async *iterMessages(): AsyncGenerator<ServerSentEvent, void, unknown> {\n    if (!this.response.body) {\n      this.controller.abort();\n      throw new Error(`Attempted to iterate over a response with no body`);\n    }\n    const lineDecoder = new LineDecoder();\n\n    const iter = readableStreamAsyncIterable<Bytes>(this.response.body);\n    for await (const chunk of iter) {\n      for (const line of lineDecoder.decode(chunk)) {\n        const sse = this.decoder.decode(line);\n        if (sse) yield sse;\n      }\n    }\n\n    for (const line of lineDecoder.flush()) {\n      const sse = this.decoder.decode(line);\n      if (sse) yield sse;\n    }\n  }\n\n  async *[Symbol.asyncIterator](): AsyncIterator<Item, any, undefined> {\n    let done = false;\n    try {\n      for await (const sse of this.iterMessages()) {\n        if (done) continue;\n\n        if (sse.data.startsWith('[DONE]')) {\n          done = true;\n          continue;\n        }\n\n        if (sse.event === null) {\n          try {\n            yield JSON.parse(sse.data);\n          } catch (e) {\n            console.error(`Could not parse message into JSON:`, sse.data);\n            console.error(`From chunk:`, sse.raw);\n            throw e;\n          }\n        }\n      }\n      done = true;\n    } catch (e) {\n      // If the user calls `stream.controller.abort()`, we should exit without throwing.\n      if (e instanceof Error && e.name === 'AbortError') return;\n      throw e;\n    } finally {\n      // If the user `break`s, abort the ongoing request.\n      if (!done) this.controller.abort();\n    }\n  }\n}\n\nclass SSEDecoder {\n  private data: string[];\n  private event: string | null;\n  private chunks: string[];\n\n  constructor() {\n    this.event = null;\n    this.data = [];\n    this.chunks = [];\n  }\n\n  decode(line: string) {\n    if (line.endsWith('\\r')) {\n      line = line.substring(0, line.length - 1);\n    }\n\n    if (!line) {\n      // empty line and we didn't previously encounter any messages\n      if (!this.event && !this.data.length) return null;\n\n      const sse: ServerSentEvent = {\n        event: this.event,\n        data: this.data.join('\\n'),\n        raw: this.chunks,\n      };\n\n      this.event = null;\n      this.data = [];\n      this.chunks = [];\n\n      return sse;\n    }\n\n    this.chunks.push(line);\n\n    if (line.startsWith(':')) {\n      return null;\n    }\n\n    let [fieldname, _, value] = partition(line, ':');\n\n    if (value.startsWith(' ')) {\n      value = value.substring(1);\n    }\n\n    if (fieldname === 'event') {\n      this.event = value;\n    } else if (fieldname === 'data') {\n      this.data.push(value);\n    }\n\n    return null;\n  }\n}\n\n/**\n * A re-implementation of httpx's `LineDecoder` in Python that handles incrementally\n * reading lines from text.\n *\n * https://github.com/encode/httpx/blob/920333ea98118e9cf617f246905d7b202510941c/httpx/_decoders.py#L258\n */\nclass LineDecoder {\n  // prettier-ignore\n  static NEWLINE_CHARS = new Set(['\\n', '\\r', '\\x0b', '\\x0c', '\\x1c', '\\x1d', '\\x1e', '\\x85', '\\u2028', '\\u2029']);\n  static NEWLINE_REGEXP = /\\r\\n|[\\n\\r\\x0b\\x0c\\x1c\\x1d\\x1e\\x85\\u2028\\u2029]/g;\n\n  buffer: string[];\n  trailingCR: boolean;\n  textDecoder: any; // TextDecoder found in browsers; not typed to avoid pulling in either \"dom\" or \"node\" types.\n\n  constructor() {\n    this.buffer = [];\n    this.trailingCR = false;\n  }\n\n  decode(chunk: Bytes): string[] {\n    let text = this.decodeText(chunk);\n\n    if (this.trailingCR) {\n      text = '\\r' + text;\n      this.trailingCR = false;\n    }\n    if (text.endsWith('\\r')) {\n      this.trailingCR = true;\n      text = text.slice(0, -1);\n    }\n\n    if (!text) {\n      return [];\n    }\n\n    const trailingNewline = LineDecoder.NEWLINE_CHARS.has(text[text.length - 1] || '');\n    let lines = text.split(LineDecoder.NEWLINE_REGEXP);\n\n    if (lines.length === 1 && !trailingNewline) {\n      this.buffer.push(lines[0]!);\n      return [];\n    }\n\n    if (this.buffer.length > 0) {\n      lines = [this.buffer.join('') + lines[0], ...lines.slice(1)];\n      this.buffer = [];\n    }\n\n    if (!trailingNewline) {\n      this.buffer = [lines.pop() || ''];\n    }\n\n    return lines;\n  }\n\n  decodeText(bytes: Bytes): string {\n    if (bytes == null) return '';\n    if (typeof bytes === 'string') return bytes;\n\n    // Node:\n    if (typeof Buffer !== 'undefined') {\n      if (bytes instanceof Buffer) {\n        return bytes.toString();\n      }\n      if (bytes instanceof Uint8Array) {\n        return Buffer.from(bytes).toString();\n      }\n\n      throw new Error(\n        `Unexpected: received non-Uint8Array (${bytes.constructor.name}) stream chunk in an environment with a global \"Buffer\" defined, which this library assumes to be Node. Please report this error.`,\n      );\n    }\n\n    // Browser\n    if (typeof TextDecoder !== 'undefined') {\n      if (bytes instanceof Uint8Array || bytes instanceof ArrayBuffer) {\n        this.textDecoder ??= new TextDecoder('utf8');\n        return this.textDecoder.decode(bytes);\n      }\n\n      throw new Error(\n        `Unexpected: received non-Uint8Array/ArrayBuffer (${\n          (bytes as any).constructor.name\n        }) in a web platform. Please report this error.`,\n      );\n    }\n\n    throw new Error(\n      `Unexpected: neither Buffer nor TextDecoder are available as globals. Please report this error.`,\n    );\n  }\n\n  flush(): string[] {\n    if (!this.buffer.length && !this.trailingCR) {\n      return [];\n    }\n\n    const lines = [this.buffer.join('')];\n    this.buffer = [];\n    this.trailingCR = false;\n    return lines;\n  }\n}\n\nfunction partition(str: string, delimiter: string): [string, string, string] {\n  const index = str.indexOf(delimiter);\n  if (index !== -1) {\n    return [str.substring(0, index), delimiter, str.substring(index + delimiter.length)];\n  }\n\n  return [str, '', ''];\n}\n\n/**\n * Most browsers don't yet have async iterable support for ReadableStream,\n * and Node has a very different way of reading bytes from its \"ReadableStream\".\n *\n * This polyfill was pulled from https://github.com/MattiasBuelens/web-streams-polyfill/pull/122#issuecomment-1627354490\n */\nfunction readableStreamAsyncIterable<T>(stream: any): AsyncIterableIterator<T> {\n  if (stream[Symbol.asyncIterator]) return stream;\n\n  const reader = stream.getReader();\n  return {\n    async next() {\n      try {\n        const result = await reader.read();\n        if (result?.done) reader.releaseLock(); // release lock when stream becomes closed\n        return result;\n      } catch (e) {\n        reader.releaseLock(); // release lock when stream becomes errored\n        throw e;\n      }\n    },\n    async return() {\n      const cancelPromise = reader.cancel();\n      reader.releaseLock();\n      await cancelPromise;\n      return { done: true, value: undefined };\n    },\n    [Symbol.asyncIterator]() {\n      return this;\n    },\n  };\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport { castToError, Headers } from './core';\n\nexport class APIError extends Error {\n  readonly status: number | undefined;\n  readonly headers: Headers | undefined;\n  readonly error: Object | undefined;\n\n  readonly code: string | null | undefined;\n  readonly param: string | null | undefined;\n  readonly type: string | undefined;\n\n  constructor(\n    status: number | undefined,\n    error: Object | undefined,\n    message: string | undefined,\n    headers: Headers | undefined,\n  ) {\n    super(APIError.makeMessage(error, message));\n    this.status = status;\n    this.headers = headers;\n\n    const data = error as Record<string, any>;\n    this.error = data;\n    this.code = data?.['code'];\n    this.param = data?.['param'];\n    this.type = data?.['type'];\n  }\n\n  private static makeMessage(error: any, message: string | undefined) {\n    return (\n      error?.message ?\n        typeof error.message === 'string' ? error.message\n        : JSON.stringify(error.message)\n      : error ? JSON.stringify(error)\n      : message || 'Unknown error occurred'\n    );\n  }\n\n  static generate(\n    status: number | undefined,\n    errorResponse: Object | undefined,\n    message: string | undefined,\n    headers: Headers | undefined,\n  ) {\n    if (!status) {\n      return new APIConnectionError({ cause: castToError(errorResponse) });\n    }\n\n    const error = (errorResponse as Record<string, any>)?.['error'];\n\n    if (status === 400) {\n      return new BadRequestError(status, error, message, headers);\n    }\n\n    if (status === 401) {\n      return new AuthenticationError(status, error, message, headers);\n    }\n\n    if (status === 403) {\n      return new PermissionDeniedError(status, error, message, headers);\n    }\n\n    if (status === 404) {\n      return new NotFoundError(status, error, message, headers);\n    }\n\n    if (status === 409) {\n      return new ConflictError(status, error, message, headers);\n    }\n\n    if (status === 422) {\n      return new UnprocessableEntityError(status, error, message, headers);\n    }\n\n    if (status === 429) {\n      return new RateLimitError(status, error, message, headers);\n    }\n\n    if (status >= 500) {\n      return new InternalServerError(status, error, message, headers);\n    }\n\n    return new APIError(status, error, message, headers);\n  }\n}\n\nexport class APIUserAbortError extends APIError {\n  override readonly status: undefined = undefined;\n\n  constructor({ message }: { message?: string } = {}) {\n    super(undefined, undefined, message || 'Request was aborted.', undefined);\n  }\n}\n\nexport class APIConnectionError extends APIError {\n  override readonly status: undefined = undefined;\n\n  constructor({ message, cause }: { message?: string; cause?: Error | undefined }) {\n    super(undefined, undefined, message || 'Connection error.', undefined);\n    // in some environments the 'cause' property is already declared\n    // @ts-ignore\n    if (cause) this.cause = cause;\n  }\n}\n\nexport class APIConnectionTimeoutError extends APIConnectionError {\n  constructor() {\n    super({ message: 'Request timed out.' });\n  }\n}\n\nexport class BadRequestError extends APIError {\n  override readonly status: 400 = 400;\n}\n\nexport class AuthenticationError extends APIError {\n  override readonly status: 401 = 401;\n}\n\nexport class PermissionDeniedError extends APIError {\n  override readonly status: 403 = 403;\n}\n\nexport class NotFoundError extends APIError {\n  override readonly status: 404 = 404;\n}\n\nexport class ConflictError extends APIError {\n  override readonly status: 409 = 409;\n}\n\nexport class UnprocessableEntityError extends APIError {\n  override readonly status: 422 = 422;\n}\n\nexport class RateLimitError extends APIError {\n  override readonly status: 429 = 429;\n}\n\nexport class InternalServerError extends APIError {}\n", "/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n *\n * This is a stub for non-node environments.\n * In node environments, it gets replaced agent.node.ts by the package export map\n */\n\nexport type Agent = any;\n\nexport const getDefaultAgent = (url: string): any => {\n  return undefined;\n};\n", "/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\n\n// If we accidentally call fetch with the wrong this binding,\n// in the browser it would throw:\n//   TypeError: Failed to execute 'fetch' on 'Window': Illegal invocation\nconst _fetch = fetch.bind(undefined);\nconst _Request = Request;\nconst _Response = Response;\nconst _Headers = Headers;\n\nexport const isPolyfilled = false;\n\nexport { _fetch as fetch, _Request as Request, _Response as Response, _Headers as Headers };\n", "/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\n\nconst _FormData = FormData;\nconst _Blob = Blob;\n\nconst _File =\n  typeof File !== 'undefined' ? File : (\n    // Bun doesn't implement File yet, so just make a shim that throws a helpful error message\n    class File extends Blob {\n      constructor() {\n        throw new Error(`file uploads aren't supported in this environment yet as 'File' is not defined`);\n      }\n    }\n  );\n\nexport { _FormData as FormData, _File as File, _Blob as Blob };\n\nexport const isPolyfilled = false;\n", "/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\n\nimport { FormData } from './formdata.js';\nimport type { RequestOptions } from '../core';\nimport { MultipartBody } from '../uploads';\n\nexport async function getMultipartRequestOptions<T extends {} = Record<string, unknown>>(\n  form: FormData,\n  opts: RequestOptions<T>,\n): Promise<RequestOptions<T>> {\n  return { ...opts, body: new MultipartBody(form) as any };\n}\n", "/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n *\n * This is a stub that gets replaced by fileFromPath.node.js for node environments\n * in the package export map\n */\n\nimport type { FilePropertyBag, File } from './formdata.js';\n\nexport type FileFromPathOptions = Omit<FilePropertyBag, 'lastModified'>;\n\n/**\n * This is a stub for non-node environments that just throws an error.\n * In node environments, this module will be replaced by util/node/fileFromPath by the\n * package import map.\n */\nexport async function fileFromPath(path: string): Promise<File>;\nexport async function fileFromPath(path: string, filename?: string): Promise<File>;\nexport async function fileFromPath(path: string, options?: FileFromPathOptions): Promise<File>;\nexport async function fileFromPath(\n  path: string,\n  filename?: string,\n  options?: FileFromPathOptions,\n): Promise<File>;\nexport async function fileFromPath(): Promise<File> {\n  throw new Error(\n    'The `fileFromPath` function is only supported in Node. See the README for more details: https://www.github.com/openai/openai-node#file-uploads',\n  );\n}\n", "/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\n\n// shim these Node types to avoid importing @types/node and polluting the user's\n// type environment in non-node projects\n\nexport declare class Readable {\n  readable: boolean;\n  readonly readableEnded: boolean;\n  readonly readableFlowing: boolean | null;\n  readonly readableHighWaterMark: number;\n  readonly readableLength: number;\n  readonly readableObjectMode: boolean;\n  destroyed: boolean;\n  read(size?: number): any;\n  pause(): this;\n  resume(): this;\n  isPaused(): boolean;\n  destroy(error?: Error): this;\n  [Symbol.asyncIterator](): AsyncIterableIterator<any>;\n}\n\nexport declare class FsReadStream extends Readable {\n  path: {}; // node type is string | Buffer\n}\n\nexport function isFsReadStream(value: any): value is FsReadStream {\n  return false;\n}\n", "import { type RequestOptions } from './core';\nimport { type Readable } from './_shims/node-readable';\nimport { type BodyInit } from './_shims/fetch.js';\nimport { FormData, File, type Blob, type FilePropertyBag } from './_shims/formdata.js';\nimport { getMultipartRequestOptions } from './_shims/getMultipartRequestOptions';\nimport { fileFromPath } from './_shims/fileFromPath';\nimport { type FsReadStream, isFsReadStream } from './_shims/node-readable';\n\nexport { fileFromPath };\n\ntype BlobLikePart = string | ArrayBuffer | ArrayBufferView | BlobLike | Uint8Array | DataView;\nexport type BlobPart = string | ArrayBuffer | ArrayBufferView | Blob | Uint8Array | DataView;\n\n/**\n * Typically, this is a native \"File\" class.\n *\n * We provide the {@link toFile} utility to convert a variety of objects\n * into the File class.\n *\n * For convenience, you can also pass a fetch Response, or in Node,\n * the result of fs.createReadStream().\n */\nexport type Uploadable = FileLike | ResponseLike | FsReadStream;\n\n/**\n * Intended to match web.Blob, node.Blob, node-fetch.Blob, etc.\n */\nexport interface BlobLike {\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/size) */\n  readonly size: number;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/type) */\n  readonly type: string;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/text) */\n  text(): Promise<string>;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/slice) */\n  slice(start?: number, end?: number): BlobLike;\n  // unfortunately @types/node-fetch@^2.6.4 doesn't type the arrayBuffer method\n}\n\n/**\n * Intended to match web.File, node.File, node-fetch.File, etc.\n */\nexport interface FileLike extends BlobLike {\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/File/lastModified) */\n  readonly lastModified: number;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/File/name) */\n  readonly name: string;\n}\n\n/**\n * Intended to match web.Response, node.Response, node-fetch.Response, etc.\n */\nexport interface ResponseLike {\n  url: string;\n  blob(): Promise<BlobLike>;\n}\n\nexport const isResponseLike = (value: any): value is ResponseLike =>\n  value != null &&\n  typeof value === 'object' &&\n  typeof value.url === 'string' &&\n  typeof value.blob === 'function';\n\nexport const isFileLike = (value: any): value is FileLike =>\n  value != null &&\n  typeof value === 'object' &&\n  typeof value.name === 'string' &&\n  typeof value.lastModified === 'number' &&\n  isBlobLike(value);\n\n/**\n * The BlobLike type omits arrayBuffer() because @types/node-fetch@^2.6.4 lacks it; but this check\n * adds the arrayBuffer() method type because it is available and used at runtime\n */\nexport const isBlobLike = (value: any): value is BlobLike & { arrayBuffer(): Promise<ArrayBuffer> } =>\n  value != null &&\n  typeof value === 'object' &&\n  typeof value.size === 'number' &&\n  typeof value.type === 'string' &&\n  typeof value.text === 'function' &&\n  typeof value.slice === 'function' &&\n  typeof value.arrayBuffer === 'function';\n\nexport const isUploadable = (value: any): value is Uploadable => {\n  return isFileLike(value) || isResponseLike(value) || isFsReadStream(value);\n};\n\nexport type ToFileInput = Uploadable | Exclude<BlobLikePart, string> | AsyncIterable<BlobLikePart>;\n\n/**\n * Helper for creating a {@link File} to pass to an SDK upload method from a variety of different data formats\n * @param value the raw content of the file.  Can be an {@link Uploadable}, {@link BlobLikePart}, or {@link AsyncIterable} of {@link BlobLikePart}s\n * @param {string=} name the name of the file. If omitted, toFile will try to determine a file name from bits if possible\n * @param {Object=} options additional properties\n * @param {string=} options.type the MIME type of the content\n * @param {number=} options.lastModified the last modified timestamp\n * @returns a {@link File} with the given properties\n */\nexport async function toFile(\n  value: ToFileInput | PromiseLike<ToFileInput>,\n  name?: string | null | undefined,\n  options: FilePropertyBag | undefined = {},\n): Promise<FileLike> {\n  // If it's a promise, resolve it.\n  value = await value;\n\n  if (isResponseLike(value)) {\n    const blob = await value.blob();\n    name ||= new URL(value.url).pathname.split(/[\\\\/]/).pop() ?? 'unknown_file';\n\n    return new File([blob as any], name, options);\n  }\n\n  const bits = await getBytes(value);\n\n  name ||= getName(value) ?? 'unknown_file';\n\n  if (!options.type) {\n    const type = (bits[0] as any)?.type;\n    if (typeof type === 'string') {\n      options = { ...options, type };\n    }\n  }\n\n  return new File(bits, name, options);\n}\n\nasync function getBytes(value: ToFileInput): Promise<Array<BlobPart>> {\n  let parts: Array<BlobPart> = [];\n  if (\n    typeof value === 'string' ||\n    ArrayBuffer.isView(value) || // includes Uint8Array, Buffer, etc.\n    value instanceof ArrayBuffer\n  ) {\n    parts.push(value);\n  } else if (isBlobLike(value)) {\n    parts.push(await value.arrayBuffer());\n  } else if (\n    isAsyncIterableIterator(value) // includes Readable, ReadableStream, etc.\n  ) {\n    for await (const chunk of value) {\n      parts.push(chunk as BlobPart); // TODO, consider validating?\n    }\n  } else {\n    throw new Error(\n      `Unexpected data type: ${typeof value}; constructor: ${\n        value?.constructor?.name\n      }; props: ${propsForError(value)}`,\n    );\n  }\n\n  return parts;\n}\n\nfunction propsForError(value: any): string {\n  const props = Object.getOwnPropertyNames(value);\n  return `[${props.map((p) => `\"${p}\"`).join(', ')}]`;\n}\n\nfunction getName(value: any): string | undefined {\n  return (\n    getStringFromMaybeBuffer(value.name) ||\n    getStringFromMaybeBuffer(value.filename) ||\n    // For fs.ReadStream\n    getStringFromMaybeBuffer(value.path)?.split(/[\\\\/]/).pop()\n  );\n}\n\nconst getStringFromMaybeBuffer = (x: string | Buffer | unknown): string | undefined => {\n  if (typeof x === 'string') return x;\n  if (typeof Buffer !== 'undefined' && x instanceof Buffer) return String(x);\n  return undefined;\n};\n\nconst isAsyncIterableIterator = (value: any): value is AsyncIterableIterator<unknown> =>\n  value != null && typeof value === 'object' && typeof value[Symbol.asyncIterator] === 'function';\n\nexport class MultipartBody {\n  constructor(public body: Readable | BodyInit) {}\n  get [Symbol.toStringTag](): string {\n    return 'MultipartBody';\n  }\n}\n\nexport const isMultipartBody = (body: any): body is MultipartBody =>\n  body && typeof body === 'object' && body.body && body[Symbol.toStringTag] === 'MultipartBody';\n\n/**\n * Returns a multipart/form-data request if any part of the given request body contains a File / Blob value.\n * Otherwise returns the request as is.\n */\nexport const maybeMultipartFormRequestOptions = async <T extends {} = Record<string, unknown>>(\n  opts: RequestOptions<T>,\n): Promise<RequestOptions<T | MultipartBody>> => {\n  if (!hasUploadableValue(opts.body)) return opts;\n\n  const form = await createForm(opts.body);\n  return getMultipartRequestOptions(form, opts);\n};\n\nexport const multipartFormRequestOptions = async <T extends {} = Record<string, unknown>>(\n  opts: RequestOptions<T>,\n): Promise<RequestOptions<T | MultipartBody>> => {\n  const form = await createForm(opts.body);\n  return getMultipartRequestOptions(form, opts);\n};\n\nexport const createForm = async <T = Record<string, unknown>>(body: T | undefined): Promise<FormData> => {\n  const form = new FormData();\n  await Promise.all(Object.entries(body || {}).map(([key, value]) => addFormValue(form, key, value)));\n  return form;\n};\n\nconst hasUploadableValue = (value: unknown): boolean => {\n  if (isUploadable(value)) return true;\n  if (Array.isArray(value)) return value.some(hasUploadableValue);\n  if (value && typeof value === 'object') {\n    for (const k in value) {\n      if (hasUploadableValue((value as any)[k])) return true;\n    }\n  }\n  return false;\n};\n\nconst addFormValue = async (form: FormData, key: string, value: unknown): Promise<void> => {\n  if (value === undefined) return;\n  if (value == null) {\n    throw new TypeError(\n      `Received null for \"${key}\"; to pass null in FormData, you must use the string 'null'`,\n    );\n  }\n\n  // TODO: make nested formats configurable\n  if (typeof value === 'string' || typeof value === 'number' || typeof value === 'boolean') {\n    form.append(key, String(value));\n  } else if (isUploadable(value)) {\n    const file = await toFile(value);\n    form.append(key, file as File);\n  } else if (Array.isArray(value)) {\n    await Promise.all(value.map((entry) => addFormValue(form, key + '[]', entry)));\n  } else if (typeof value === 'object') {\n    await Promise.all(\n      Object.entries(value).map(([name, prop]) => addFormValue(form, `${key}[${name}]`, prop)),\n    );\n  } else {\n    throw new TypeError(\n      `Invalid value given to form, expected a string, number, boolean, object, Array, File or Blob but got ${value} instead`,\n    );\n  }\n};\n", "import { VERSION } from './version';\nimport { Stream } from './streaming';\nimport { APIError, APIConnectionError, APIConnectionTimeoutError, APIUserAbortError } from './error';\nimport type { Readable } from './_shims/node-readable';\nimport { getDefaultAgent, type Agent } from './_shims/agent';\nimport {\n  fetch,\n  isPolyfilled as fetchIsPolyfilled,\n  type RequestInfo,\n  type RequestInit,\n  type Response,\n} from './_shims/fetch.js';\nexport { type Response };\nimport { isMultipartBody } from './uploads';\nexport {\n  maybeMultipartFormRequestOptions,\n  multipartFormRequestOptions,\n  createForm,\n  type Uploadable,\n} from './uploads';\n\nconst MAX_RETRIES = 2;\n\nexport type Fetch = (url: RequestInfo, init?: RequestInit) => Promise<Response>;\n\ntype PromiseOrValue<T> = T | Promise<T>;\n\ntype APIResponseProps = {\n  response: Response;\n  options: FinalRequestOptions;\n  controller: AbortController;\n};\n\nasync function defaultParseResponse<T>(props: APIResponseProps): Promise<T> {\n  const { response } = props;\n  if (props.options.stream) {\n    // Note: there is an invariant here that isn't represented in the type system\n    // that if you set `stream: true` the response type must also be `Stream<T>`\n    return new Stream(response, props.controller) as any;\n  }\n\n  const contentType = response.headers.get('content-type');\n  if (contentType?.includes('application/json')) {\n    const json = await response.json();\n\n    debug('response', response.status, response.url, response.headers, json);\n\n    return json as T;\n  }\n\n  // TODO handle blob, arraybuffer, other content types, etc.\n  const text = await response.text();\n  debug('response', response.status, response.url, response.headers, text);\n  return text as T;\n}\n\n/**\n * A subclass of `Promise` providing additional helper methods\n * for interacting with the SDK.\n */\nexport class APIPromise<T> extends Promise<T> {\n  private parsedPromise: Promise<T> | undefined;\n\n  constructor(\n    private responsePromise: Promise<APIResponseProps>,\n    private parseResponse: (props: APIResponseProps) => PromiseOrValue<T> = defaultParseResponse,\n  ) {\n    super((resolve) => {\n      // this is maybe a bit weird but this has to be a no-op to not implicitly\n      // parse the response body; instead .then, .catch, .finally are overridden\n      // to parse the response\n      resolve(null as any);\n    });\n  }\n\n  _thenUnwrap<U>(transform: (data: T) => U): APIPromise<U> {\n    return new APIPromise(this.responsePromise, async (props) => transform(await this.parseResponse(props)));\n  }\n\n  /**\n   * Gets the raw `Response` instance instead of parsing the response\n   * data.\n   *\n   * If you want to parse the response body but still get the `Response`\n   * instance, you can use {@link withResponse()}.\n   */\n  asResponse(): Promise<Response> {\n    return this.responsePromise.then((p) => p.response);\n  }\n  /**\n   * Gets the parsed response data and the raw `Response` instance.\n   *\n   * If you just want to get the raw `Response` instance without parsing it,\n   * you can use {@link asResponse()}.\n   */\n  async withResponse(): Promise<{ data: T; response: Response }> {\n    const [data, response] = await Promise.all([this.parse(), this.asResponse()]);\n    return { data, response };\n  }\n\n  private parse(): Promise<T> {\n    if (!this.parsedPromise) {\n      this.parsedPromise = this.responsePromise.then(this.parseResponse);\n    }\n    return this.parsedPromise;\n  }\n\n  override then<TResult1 = T, TResult2 = never>(\n    onfulfilled?: ((value: T) => TResult1 | PromiseLike<TResult1>) | undefined | null,\n    onrejected?: ((reason: any) => TResult2 | PromiseLike<TResult2>) | undefined | null,\n  ): Promise<TResult1 | TResult2> {\n    return this.parse().then(onfulfilled, onrejected);\n  }\n\n  override catch<TResult = never>(\n    onrejected?: ((reason: any) => TResult | PromiseLike<TResult>) | undefined | null,\n  ): Promise<T | TResult> {\n    return this.parse().catch(onrejected);\n  }\n\n  override finally(onfinally?: (() => void) | undefined | null): Promise<T> {\n    return this.parse().finally(onfinally);\n  }\n}\n\nexport abstract class APIClient {\n  baseURL: string;\n  maxRetries: number;\n  timeout: number;\n  httpAgent: Agent | undefined;\n\n  private fetch: Fetch;\n  protected idempotencyHeader?: string;\n\n  constructor({\n    baseURL,\n    maxRetries,\n    timeout = 600000, // 10 minutes\n    httpAgent,\n    fetch: overridenFetch,\n  }: {\n    baseURL: string;\n    maxRetries?: number | undefined;\n    timeout: number | undefined;\n    httpAgent: Agent | undefined;\n    fetch: Fetch | undefined;\n  }) {\n    this.baseURL = baseURL;\n    this.maxRetries = validatePositiveInteger('maxRetries', maxRetries ?? MAX_RETRIES);\n    this.timeout = validatePositiveInteger('timeout', timeout);\n    this.httpAgent = httpAgent;\n\n    this.fetch = overridenFetch ?? fetch;\n  }\n\n  protected authHeaders(): Headers {\n    return {};\n  }\n\n  /**\n   * Override this to add your own default headers, for example:\n   *\n   *  {\n   *    ...super.defaultHeaders(),\n   *    Authorization: 'Bearer 123',\n   *  }\n   */\n  protected defaultHeaders(): Headers {\n    return {\n      Accept: 'application/json',\n      'Content-Type': 'application/json',\n      'User-Agent': this.getUserAgent(),\n      ...getPlatformHeaders(),\n      ...this.authHeaders(),\n    };\n  }\n\n  protected abstract defaultQuery(): DefaultQuery | undefined;\n\n  /**\n   * Override this to add your own headers validation:\n   */\n  protected validateHeaders(headers: Headers, customHeaders: Headers) {}\n\n  protected defaultIdempotencyKey(): string {\n    return `stainless-node-retry-${uuid4()}`;\n  }\n\n  get<Req extends {}, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('get', path, opts);\n  }\n\n  post<Req extends {}, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('post', path, opts);\n  }\n\n  patch<Req extends {}, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('patch', path, opts);\n  }\n\n  put<Req extends {}, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('put', path, opts);\n  }\n\n  delete<Req extends {}, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('delete', path, opts);\n  }\n\n  private methodRequest<Req extends {}, Rsp>(\n    method: HTTPMethod,\n    path: string,\n    opts?: PromiseOrValue<RequestOptions<Req>>,\n  ): APIPromise<Rsp> {\n    return this.request(Promise.resolve(opts).then((opts) => ({ method, path, ...opts })));\n  }\n\n  getAPIList<Item, PageClass extends AbstractPage<Item> = AbstractPage<Item>>(\n    path: string,\n    Page: new (...args: any[]) => PageClass,\n    opts?: RequestOptions<any>,\n  ): PagePromise<PageClass, Item> {\n    return this.requestAPIList(Page, { method: 'get', path, ...opts });\n  }\n\n  private calculateContentLength(body: unknown): string | null {\n    if (typeof body === 'string') {\n      if (typeof Buffer !== 'undefined') {\n        return Buffer.byteLength(body, 'utf8').toString();\n      }\n\n      if (typeof TextEncoder !== 'undefined') {\n        const encoder = new TextEncoder();\n        const encoded = encoder.encode(body);\n        return encoded.length.toString();\n      }\n    }\n\n    return null;\n  }\n\n  buildRequest<Req extends {}>(\n    options: FinalRequestOptions<Req>,\n  ): { req: RequestInit; url: string; timeout: number } {\n    const { method, path, query, headers: headers = {} } = options;\n\n    const body =\n      isMultipartBody(options.body) ? options.body.body\n      : options.body ? JSON.stringify(options.body, null, 2)\n      : null;\n    const contentLength = this.calculateContentLength(body);\n\n    const url = this.buildURL(path!, query);\n    if ('timeout' in options) validatePositiveInteger('timeout', options.timeout);\n    const timeout = options.timeout ?? this.timeout;\n    const httpAgent = options.httpAgent ?? this.httpAgent ?? getDefaultAgent(url);\n    const minAgentTimeout = timeout + 1000;\n    if (\n      typeof (httpAgent as any)?.options?.timeout === 'number' &&\n      minAgentTimeout > ((httpAgent as any).options.timeout ?? 0)\n    ) {\n      // Allow any given request to bump our agent active socket timeout.\n      // This may seem strange, but leaking active sockets should be rare and not particularly problematic,\n      // and without mutating agent we would need to create more of them.\n      // This tradeoff optimizes for performance.\n      (httpAgent as any).options.timeout = minAgentTimeout;\n    }\n\n    if (this.idempotencyHeader && method !== 'get') {\n      if (!options.idempotencyKey) options.idempotencyKey = this.defaultIdempotencyKey();\n      headers[this.idempotencyHeader] = options.idempotencyKey;\n    }\n\n    const reqHeaders: Record<string, string> = {\n      ...(contentLength && { 'Content-Length': contentLength }),\n      ...this.defaultHeaders(),\n      ...headers,\n    };\n    // let builtin fetch set the Content-Type for multipart bodies\n    if (isMultipartBody(options.body) && !fetchIsPolyfilled) {\n      delete reqHeaders['Content-Type'];\n    }\n\n    // Strip any headers being explicitly omitted with null\n    Object.keys(reqHeaders).forEach((key) => reqHeaders[key] === null && delete reqHeaders[key]);\n\n    const req: RequestInit = {\n      method,\n      ...(body && { body: body as any }),\n      headers: reqHeaders,\n      ...(httpAgent && { agent: httpAgent }),\n      // @ts-ignore node-fetch uses a custom AbortSignal type that is\n      // not compatible with standard web types\n      signal: options.signal ?? null,\n    };\n\n    this.validateHeaders(reqHeaders, headers);\n\n    return { req, url, timeout };\n  }\n\n  /**\n   * Used as a callback for mutating the given `RequestInit` object.\n   *\n   * This is useful for cases where you want to add certain headers based off of\n   * the request properties, e.g. `method` or `url`.\n   */\n  protected async prepareRequest(request: RequestInit, { url }: { url: string }): Promise<void> {}\n\n  protected makeStatusError(\n    status: number | undefined,\n    error: Object | undefined,\n    message: string | undefined,\n    headers: Headers | undefined,\n  ) {\n    return APIError.generate(status, error, message, headers);\n  }\n\n  request<Req extends {}, Rsp>(\n    options: PromiseOrValue<FinalRequestOptions<Req>>,\n    remainingRetries: number | null = null,\n  ): APIPromise<Rsp> {\n    return new APIPromise(this.makeRequest(options, remainingRetries));\n  }\n\n  private async makeRequest(\n    optionsInput: PromiseOrValue<FinalRequestOptions>,\n    retriesRemaining: number | null,\n  ): Promise<APIResponseProps> {\n    const options = await optionsInput;\n    if (retriesRemaining == null) {\n      retriesRemaining = options.maxRetries ?? this.maxRetries;\n    }\n\n    const { req, url, timeout } = this.buildRequest(options);\n\n    await this.prepareRequest(req, { url });\n\n    debug('request', url, options, req.headers);\n\n    if (options.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n\n    const controller = new AbortController();\n    const response = await this.fetchWithTimeout(url, req, timeout, controller).catch(castToError);\n\n    if (response instanceof Error) {\n      if (options.signal?.aborted) {\n        throw new APIUserAbortError();\n      }\n      if (retriesRemaining) {\n        return this.retryRequest(options, retriesRemaining);\n      }\n      if (response.name === 'AbortError') {\n        throw new APIConnectionTimeoutError();\n      }\n      throw new APIConnectionError({ cause: response });\n    }\n\n    const responseHeaders = createResponseHeaders(response.headers);\n\n    if (!response.ok) {\n      if (retriesRemaining && this.shouldRetry(response)) {\n        return this.retryRequest(options, retriesRemaining, responseHeaders);\n      }\n\n      const errText = await response.text().catch(() => 'Unknown');\n      const errJSON = safeJSON(errText);\n      const errMessage = errJSON ? undefined : errText;\n\n      debug('response', response.status, url, responseHeaders, errMessage);\n\n      const err = this.makeStatusError(response.status, errJSON, errMessage, responseHeaders);\n      throw err;\n    }\n\n    return { response, options, controller };\n  }\n\n  requestAPIList<Item = unknown, PageClass extends AbstractPage<Item> = AbstractPage<Item>>(\n    Page: new (...args: ConstructorParameters<typeof AbstractPage>) => PageClass,\n    options: FinalRequestOptions,\n  ): PagePromise<PageClass, Item> {\n    const request = this.makeRequest(options, null);\n    return new PagePromise<PageClass, Item>(this, request, Page);\n  }\n\n  buildURL<Req>(path: string, query: Req | undefined): string {\n    const url =\n      isAbsoluteURL(path) ?\n        new URL(path)\n      : new URL(this.baseURL + (this.baseURL.endsWith('/') && path.startsWith('/') ? path.slice(1) : path));\n\n    const defaultQuery = this.defaultQuery();\n    if (!isEmptyObj(defaultQuery)) {\n      query = { ...defaultQuery, ...query } as Req;\n    }\n\n    if (query) {\n      url.search = this.stringifyQuery(query);\n    }\n\n    return url.toString();\n  }\n\n  protected stringifyQuery(query: Record<string, unknown>): string {\n    return Object.entries(query)\n      .filter(([_, value]) => typeof value !== 'undefined')\n      .map(([key, value]) => {\n        if (typeof value === 'string' || typeof value === 'number' || typeof value === 'boolean') {\n          return `${encodeURIComponent(key)}=${encodeURIComponent(value)}`;\n        }\n        if (value === null) {\n          return `${encodeURIComponent(key)}=`;\n        }\n        throw new Error(\n          `Cannot stringify type ${typeof value}; Expected string, number, boolean, or null. If you need to pass nested query parameters, you can manually encode them, e.g. { query: { 'foo[key1]': value1, 'foo[key2]': value2 } }, and please open a GitHub issue requesting better support for your use case.`,\n        );\n      })\n      .join('&');\n  }\n\n  async fetchWithTimeout(\n    url: RequestInfo,\n    init: RequestInit | undefined,\n    ms: number,\n    controller: AbortController,\n  ): Promise<Response> {\n    const { signal, ...options } = init || {};\n    if (signal) signal.addEventListener('abort', () => controller.abort());\n\n    const timeout = setTimeout(() => controller.abort(), ms);\n\n    return this.getRequestClient()\n      .fetch(url, { signal: controller.signal as any, ...options })\n      .finally(() => {\n        clearTimeout(timeout);\n      });\n  }\n\n  protected getRequestClient(): RequestClient {\n    return { fetch: this.fetch };\n  }\n\n  private shouldRetry(response: Response): boolean {\n    // Note this is not a standard header.\n    const shouldRetryHeader = response.headers.get('x-should-retry');\n\n    // If the server explicitly says whether or not to retry, obey.\n    if (shouldRetryHeader === 'true') return true;\n    if (shouldRetryHeader === 'false') return false;\n\n    // Retry on lock timeouts.\n    if (response.status === 409) return true;\n\n    // Retry on rate limits.\n    if (response.status === 429) return true;\n\n    // Retry internal errors.\n    if (response.status >= 500) return true;\n\n    return false;\n  }\n\n  private async retryRequest(\n    options: FinalRequestOptions,\n    retriesRemaining: number,\n    responseHeaders?: Headers | undefined,\n  ): Promise<APIResponseProps> {\n    retriesRemaining -= 1;\n\n    // About the Retry-After header: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Retry-After\n    //\n    // TODO: we may want to handle the case where the header is using the http-date syntax: \"Retry-After: <http-date>\".\n    // See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Retry-After#syntax for details.\n    const retryAfter = parseInt(responseHeaders?.['retry-after'] || '');\n\n    const maxRetries = options.maxRetries ?? this.maxRetries;\n    const timeout = this.calculateRetryTimeoutSeconds(retriesRemaining, retryAfter, maxRetries) * 1000;\n    await sleep(timeout);\n\n    return this.makeRequest(options, retriesRemaining);\n  }\n\n  private calculateRetryTimeoutSeconds(\n    retriesRemaining: number,\n    retryAfter: number,\n    maxRetries: number,\n  ): number {\n    const initialRetryDelay = 0.5;\n    const maxRetryDelay = 2;\n\n    // If the API asks us to wait a certain amount of time (and it's a reasonable amount),\n    // just do what it says.\n    if (Number.isInteger(retryAfter) && retryAfter <= 60) {\n      return retryAfter;\n    }\n\n    const numRetries = maxRetries - retriesRemaining;\n\n    // Apply exponential backoff, but not more than the max.\n    const sleepSeconds = Math.min(initialRetryDelay * Math.pow(numRetries - 1, 2), maxRetryDelay);\n\n    // Apply some jitter, plus-or-minus half a second.\n    const jitter = Math.random() - 0.5;\n\n    return sleepSeconds + jitter;\n  }\n\n  private getUserAgent(): string {\n    return `${this.constructor.name}/JS ${VERSION}`;\n  }\n}\n\nexport class APIResource {\n  protected client: APIClient;\n  constructor(client: APIClient) {\n    this.client = client;\n\n    this.get = client.get.bind(client);\n    this.post = client.post.bind(client);\n    this.patch = client.patch.bind(client);\n    this.put = client.put.bind(client);\n    this.delete = client.delete.bind(client);\n    this.getAPIList = client.getAPIList.bind(client);\n  }\n\n  protected get: APIClient['get'];\n  protected post: APIClient['post'];\n  protected patch: APIClient['patch'];\n  protected put: APIClient['put'];\n  protected delete: APIClient['delete'];\n  protected getAPIList: APIClient['getAPIList'];\n}\n\nexport type PageInfo = { url: URL } | { params: Record<string, unknown> | null };\n\nexport abstract class AbstractPage<Item> implements AsyncIterable<Item> {\n  #client: APIClient;\n  protected options: FinalRequestOptions;\n\n  protected response: Response;\n  protected body: unknown;\n\n  constructor(client: APIClient, response: Response, body: unknown, options: FinalRequestOptions) {\n    this.#client = client;\n    this.options = options;\n    this.response = response;\n    this.body = body;\n  }\n\n  /**\n   * @deprecated Use nextPageInfo instead\n   */\n  abstract nextPageParams(): Partial<Record<string, unknown>> | null;\n  abstract nextPageInfo(): PageInfo | null;\n\n  abstract getPaginatedItems(): Item[];\n\n  hasNextPage(): boolean {\n    const items = this.getPaginatedItems();\n    if (!items.length) return false;\n    return this.nextPageInfo() != null;\n  }\n\n  async getNextPage(): Promise<AbstractPage<Item>> {\n    const nextInfo = this.nextPageInfo();\n    if (!nextInfo) {\n      throw new Error(\n        'No next page expected; please check `.hasNextPage()` before calling `.getNextPage()`.',\n      );\n    }\n    const nextOptions = { ...this.options };\n    if ('params' in nextInfo) {\n      nextOptions.query = { ...nextOptions.query, ...nextInfo.params };\n    } else if ('url' in nextInfo) {\n      const params = [...Object.entries(nextOptions.query || {}), ...nextInfo.url.searchParams.entries()];\n      for (const [key, value] of params) {\n        nextInfo.url.searchParams.set(key, value);\n      }\n      nextOptions.query = undefined;\n      nextOptions.path = nextInfo.url.toString();\n    }\n    return await this.#client.requestAPIList(this.constructor as any, nextOptions);\n  }\n\n  async *iterPages() {\n    // eslint-disable-next-line @typescript-eslint/no-this-alias\n    let page: AbstractPage<Item> = this;\n    yield page;\n    while (page.hasNextPage()) {\n      page = await page.getNextPage();\n      yield page;\n    }\n  }\n\n  async *[Symbol.asyncIterator]() {\n    for await (const page of this.iterPages()) {\n      for (const item of page.getPaginatedItems()) {\n        yield item;\n      }\n    }\n  }\n}\n\n/**\n * This subclass of Promise will resolve to an instantiated Page once the request completes.\n *\n * It also implements AsyncIterable to allow auto-paginating iteration on an unawaited list call, eg:\n *\n *    for await (const item of client.items.list()) {\n *      console.log(item)\n *    }\n */\nexport class PagePromise<\n    PageClass extends AbstractPage<Item>,\n    Item = ReturnType<PageClass['getPaginatedItems']>[number],\n  >\n  extends APIPromise<PageClass>\n  implements AsyncIterable<Item>\n{\n  constructor(\n    client: APIClient,\n    request: Promise<APIResponseProps>,\n    Page: new (...args: ConstructorParameters<typeof AbstractPage>) => PageClass,\n  ) {\n    super(\n      request,\n      async (props) => new Page(client, props.response, await defaultParseResponse(props), props.options),\n    );\n  }\n\n  /**\n   * Allow auto-paginating iteration on an unawaited list call, eg:\n   *\n   *    for await (const item of client.items.list()) {\n   *      console.log(item)\n   *    }\n   */\n  async *[Symbol.asyncIterator]() {\n    const page = await this;\n    for await (const item of page) {\n      yield item;\n    }\n  }\n}\n\nexport const createResponseHeaders = (\n  headers: Awaited<ReturnType<Fetch>>['headers'],\n): Record<string, string> => {\n  return new Proxy(\n    Object.fromEntries(\n      // @ts-ignore\n      headers.entries(),\n    ),\n    {\n      get(target, name) {\n        const key = name.toString();\n        return target[key.toLowerCase()] || target[key];\n      },\n    },\n  );\n};\n\ntype HTTPMethod = 'get' | 'post' | 'put' | 'patch' | 'delete';\n\nexport type RequestClient = { fetch: Fetch };\nexport type Headers = Record<string, string | null | undefined>;\nexport type DefaultQuery = Record<string, string | undefined>;\nexport type KeysEnum<T> = { [P in keyof Required<T>]: true };\n\nexport type RequestOptions<Req extends {} = Record<string, unknown> | Readable> = {\n  method?: HTTPMethod;\n  path?: string;\n  query?: Req | undefined;\n  body?: Req | undefined;\n  headers?: Headers | undefined;\n\n  maxRetries?: number;\n  stream?: boolean | undefined;\n  timeout?: number;\n  httpAgent?: Agent;\n  signal?: AbortSignal | undefined | null;\n  idempotencyKey?: string;\n};\n\n// This is required so that we can determine if a given object matches the RequestOptions\n// type at runtime. While this requires duplication, it is enforced by the TypeScript\n// compiler such that any missing / extraneous keys will cause an error.\nconst requestOptionsKeys: KeysEnum<RequestOptions> = {\n  method: true,\n  path: true,\n  query: true,\n  body: true,\n  headers: true,\n\n  maxRetries: true,\n  stream: true,\n  timeout: true,\n  httpAgent: true,\n  signal: true,\n  idempotencyKey: true,\n};\n\nexport const isRequestOptions = (obj: unknown): obj is RequestOptions => {\n  return (\n    typeof obj === 'object' &&\n    obj !== null &&\n    !isEmptyObj(obj) &&\n    Object.keys(obj).every((k) => hasOwn(requestOptionsKeys, k))\n  );\n};\n\nexport type FinalRequestOptions<Req extends {} = Record<string, unknown> | Readable> = RequestOptions<Req> & {\n  method: HTTPMethod;\n  path: string;\n};\n\ndeclare const Deno: any;\ndeclare const EdgeRuntime: any;\ntype Arch = 'x32' | 'x64' | 'arm' | 'arm64' | `other:${string}` | 'unknown';\ntype PlatformName =\n  | 'MacOS'\n  | 'Linux'\n  | 'Windows'\n  | 'FreeBSD'\n  | 'OpenBSD'\n  | 'iOS'\n  | 'Android'\n  | `Other:${string}`\n  | 'Unknown';\ntype Browser = 'ie' | 'edge' | 'chrome' | 'firefox' | 'safari';\ntype PlatformProperties = {\n  'X-Stainless-Lang': 'js';\n  'X-Stainless-Package-Version': string;\n  'X-Stainless-OS': PlatformName;\n  'X-Stainless-Arch': Arch;\n  'X-Stainless-Runtime': 'node' | 'deno' | 'edge' | `browser:${Browser}` | 'unknown';\n  'X-Stainless-Runtime-Version': string;\n};\nconst getPlatformProperties = (): PlatformProperties => {\n  if (typeof Deno !== 'undefined' && Deno.build != null) {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': normalizePlatform(Deno.build.os),\n      'X-Stainless-Arch': normalizeArch(Deno.build.arch),\n      'X-Stainless-Runtime': 'deno',\n      'X-Stainless-Runtime-Version': Deno.version,\n    };\n  }\n  if (typeof EdgeRuntime !== 'undefined') {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': 'Unknown',\n      'X-Stainless-Arch': `other:${EdgeRuntime}`,\n      'X-Stainless-Runtime': 'edge',\n      'X-Stainless-Runtime-Version': process.version,\n    };\n  }\n  // Check if Node.js\n  if (Object.prototype.toString.call(typeof process !== 'undefined' ? process : 0) === '[object process]') {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': normalizePlatform(process.platform),\n      'X-Stainless-Arch': normalizeArch(process.arch),\n      'X-Stainless-Runtime': 'node',\n      'X-Stainless-Runtime-Version': process.version,\n    };\n  }\n\n  const browserInfo = getBrowserInfo();\n  if (browserInfo) {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': 'Unknown',\n      'X-Stainless-Arch': 'unknown',\n      'X-Stainless-Runtime': `browser:${browserInfo.browser}`,\n      'X-Stainless-Runtime-Version': browserInfo.version,\n    };\n  }\n\n  // TODO add support for Cloudflare workers, etc.\n  return {\n    'X-Stainless-Lang': 'js',\n    'X-Stainless-Package-Version': VERSION,\n    'X-Stainless-OS': 'Unknown',\n    'X-Stainless-Arch': 'unknown',\n    'X-Stainless-Runtime': 'unknown',\n    'X-Stainless-Runtime-Version': 'unknown',\n  };\n};\n\ntype BrowserInfo = {\n  browser: Browser;\n  version: string;\n};\n\ndeclare const navigator: { userAgent: string } | undefined;\n\n// Note: modified from https://github.com/JS-DevTools/host-environment/blob/b1ab79ecde37db5d6e163c050e54fe7d287d7c92/src/isomorphic.browser.ts\nfunction getBrowserInfo(): BrowserInfo | null {\n  if (!navigator || typeof navigator === 'undefined') {\n    return null;\n  }\n\n  // NOTE: The order matters here!\n  const browserPatterns = [\n    { key: 'edge' as const, pattern: /Edge(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'ie' as const, pattern: /MSIE(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'ie' as const, pattern: /Trident(?:.*rv\\:(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'chrome' as const, pattern: /Chrome(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'firefox' as const, pattern: /Firefox(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'safari' as const, pattern: /(?:Version\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?(?:\\W+Mobile\\S*)?\\W+Safari/ },\n  ];\n\n  // Find the FIRST matching browser\n  for (const { key, pattern } of browserPatterns) {\n    const match = pattern.exec(navigator.userAgent);\n    if (match) {\n      const major = match[1] || 0;\n      const minor = match[2] || 0;\n      const patch = match[3] || 0;\n\n      return { browser: key, version: `${major}.${minor}.${patch}` };\n    }\n  }\n\n  return null;\n}\n\nconst normalizeArch = (arch: string): Arch => {\n  // Node docs:\n  // - https://nodejs.org/api/process.html#processarch\n  // Deno docs:\n  // - https://doc.deno.land/deno/stable/~/Deno.build\n  if (arch === 'x32') return 'x32';\n  if (arch === 'x86_64' || arch === 'x64') return 'x64';\n  if (arch === 'arm') return 'arm';\n  if (arch === 'aarch64' || arch === 'arm64') return 'arm64';\n  if (arch) return `other:${arch}`;\n  return 'unknown';\n};\n\nconst normalizePlatform = (platform: string): PlatformName => {\n  // Node platforms:\n  // - https://nodejs.org/api/process.html#processplatform\n  // Deno platforms:\n  // - https://doc.deno.land/deno/stable/~/Deno.build\n  // - https://github.com/denoland/deno/issues/14799\n\n  platform = platform.toLowerCase();\n\n  // NOTE: this iOS check is untested and may not work\n  // Node does not work natively on IOS, there is a fork at\n  // https://github.com/nodejs-mobile/nodejs-mobile\n  // however it is unknown at the time of writing how to detect if it is running\n  if (platform.includes('ios')) return 'iOS';\n  if (platform === 'android') return 'Android';\n  if (platform === 'darwin') return 'MacOS';\n  if (platform === 'win32') return 'Windows';\n  if (platform === 'freebsd') return 'FreeBSD';\n  if (platform === 'openbsd') return 'OpenBSD';\n  if (platform === 'linux') return 'Linux';\n  if (platform) return `Other:${platform}`;\n  return 'Unknown';\n};\n\nlet _platformHeaders: PlatformProperties;\nconst getPlatformHeaders = () => {\n  return (_platformHeaders ??= getPlatformProperties());\n};\n\nexport const safeJSON = (text: string) => {\n  try {\n    return JSON.parse(text);\n  } catch (err) {\n    return undefined;\n  }\n};\n\n// https://stackoverflow.com/a/19709846\nconst startsWithSchemeRegexp = new RegExp('^(?:[a-z]+:)?//', 'i');\nconst isAbsoluteURL = (url: string): boolean => {\n  return startsWithSchemeRegexp.test(url);\n};\n\nconst sleep = (ms: number) => new Promise((resolve) => setTimeout(resolve, ms));\n\nconst validatePositiveInteger = (name: string, n: unknown): number => {\n  if (typeof n !== 'number' || !Number.isInteger(n)) {\n    throw new Error(`${name} must be an integer`);\n  }\n  if (n < 0) {\n    throw new Error(`${name} must be a positive integer`);\n  }\n  return n;\n};\n\nexport const castToError = (err: any): Error => {\n  if (err instanceof Error) return err;\n  return new Error(err);\n};\n\nexport const ensurePresent = <T>(value: T | null | undefined): T => {\n  if (value == null) throw new Error(`Expected a value to be given but received ${value} instead.`);\n  return value;\n};\n\n/**\n * Read an environment variable.\n *\n * Will return undefined if the environment variable doesn't exist or cannot be accessed.\n */\nexport const readEnv = (env: string): string | undefined => {\n  if (typeof process !== 'undefined') {\n    return process.env?.[env] ?? undefined;\n  }\n  if (typeof Deno !== 'undefined') {\n    return Deno.env?.get?.(env);\n  }\n  return undefined;\n};\n\nexport const coerceInteger = (value: unknown): number => {\n  if (typeof value === 'number') return Math.round(value);\n  if (typeof value === 'string') return parseInt(value, 10);\n\n  throw new Error(`Could not coerce ${value} (type: ${typeof value}) into a number`);\n};\n\nexport const coerceFloat = (value: unknown): number => {\n  if (typeof value === 'number') return value;\n  if (typeof value === 'string') return parseFloat(value);\n\n  throw new Error(`Could not coerce ${value} (type: ${typeof value}) into a number`);\n};\n\nexport const coerceBoolean = (value: unknown): boolean => {\n  if (typeof value === 'boolean') return value;\n  if (typeof value === 'string') return value === 'true';\n  return Boolean(value);\n};\n\nexport const maybeCoerceInteger = (value: unknown): number | undefined => {\n  if (value === undefined) {\n    return undefined;\n  }\n  return coerceInteger(value);\n};\n\nexport const maybeCoerceFloat = (value: unknown): number | undefined => {\n  if (value === undefined) {\n    return undefined;\n  }\n  return coerceFloat(value);\n};\n\nexport const maybeCoerceBoolean = (value: unknown): boolean | undefined => {\n  if (value === undefined) {\n    return undefined;\n  }\n  return coerceBoolean(value);\n};\n\n// https://stackoverflow.com/a/34491287\nexport function isEmptyObj(obj: Object | null | undefined): boolean {\n  if (!obj) return true;\n  for (const _k in obj) return false;\n  return true;\n}\n\n// https://eslint.org/docs/latest/rules/no-prototype-builtins\nexport function hasOwn(obj: Object, key: string): boolean {\n  return Object.prototype.hasOwnProperty.call(obj, key);\n}\n\nexport function debug(action: string, ...args: any[]) {\n  if (typeof process !== 'undefined' && process.env['DEBUG'] === 'true') {\n    console.log(`OpenAI:DEBUG:${action}`, ...args);\n  }\n}\n\n/**\n * https://stackoverflow.com/a/2117523\n */\nconst uuid4 = () => {\n  return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, (c) => {\n    const r = (Math.random() * 16) | 0;\n    const v = c === 'x' ? r : (r & 0x3) | 0x8;\n    return v.toString(16);\n  });\n};\n\nexport const isRunningInBrowser = () => {\n  return (\n    // @ts-ignore\n    typeof window !== 'undefined' &&\n    // @ts-ignore\n    typeof window.document !== 'undefined' &&\n    // @ts-ignore\n    typeof navigator !== 'undefined'\n  );\n};\n\nexport interface HeadersProtocol {\n  get: (header: string) => string | null | undefined;\n}\nexport type HeadersLike = Record<string, string | string[] | undefined> | HeadersProtocol;\n\nexport const isHeadersProtocol = (headers: any): headers is HeadersProtocol => {\n  return typeof headers?.get === 'function';\n};\n\nexport const getHeader = (headers: HeadersLike, key: string): string | null | undefined => {\n  const lowerKey = key.toLowerCase();\n  if (isHeadersProtocol(headers)) return headers.get(key) || headers.get(lowerKey);\n  const value = headers[key] || headers[lowerKey];\n  if (Array.isArray(value)) {\n    if (value.length <= 1) return value[0];\n    console.warn(`Received ${value.length} entries for the ${key} header, using the first entry.`);\n    return value[0];\n  }\n  return value;\n};\n\n/**\n * Encodes a string to Base64 format.\n */\nexport const toBase64 = (str: string | null | undefined): string => {\n  if (!str) return '';\n  if (typeof Buffer !== 'undefined') {\n    return Buffer.from(str).toString('base64');\n  }\n\n  if (typeof btoa !== 'undefined') {\n    return btoa(str);\n  }\n\n  throw new Error('Cannot generate b64 string; Expected `Buffer` or `btoa` to be defined');\n};\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport { AbstractPage, Response, APIClient, FinalRequestOptions } from './core';\n\nexport interface PageResponse<Item> {\n  data: Array<Item>;\n\n  object: string;\n}\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class Page<Item> extends AbstractPage<Item> implements PageResponse<Item> {\n  object: string;\n\n  data: Array<Item>;\n\n  constructor(client: APIClient, response: Response, body: PageResponse<Item>, options: FinalRequestOptions) {\n    super(client, response, body, options);\n\n    this.object = body.object;\n    this.data = body.data;\n  }\n\n  getPaginatedItems(): Item[] {\n    return this.data;\n  }\n\n  // @deprecated Please use `nextPageInfo()` instead\n  /**\n   * This page represents a response that isn't actually paginated at the API level\n   * so there will never be any next page params.\n   */\n  nextPageParams(): null {\n    return null;\n  }\n\n  nextPageInfo(): null {\n    return null;\n  }\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport type { OpenAI } from './index';\n\nexport class APIResource {\n  protected client: OpenAI;\n  constructor(client: OpenAI) {\n    this.client = client;\n\n    this.get = client.get.bind(client);\n    this.post = client.post.bind(client);\n    this.patch = client.patch.bind(client);\n    this.put = client.put.bind(client);\n    this.delete = client.delete.bind(client);\n    this.getAPIList = client.getAPIList.bind(client);\n  }\n\n  protected get: OpenAI['get'];\n  protected post: OpenAI['post'];\n  protected patch: OpenAI['patch'];\n  protected put: OpenAI['put'];\n  protected delete: OpenAI['delete'];\n  protected getAPIList: OpenAI['getAPIList'];\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../../core';\nimport { APIResource } from '../../resource';\nimport * as API from './index';\nimport { type Uploadable, multipartFormRequestOptions } from '../../core';\n\nexport class Transcriptions extends APIResource {\n  /**\n   * Transcribes audio into the input language.\n   */\n  create(body: TranscriptionCreateParams, options?: Core.RequestOptions): Core.APIPromise<Transcription> {\n    return this.post('/audio/transcriptions', multipartFormRequestOptions({ body, ...options }));\n  }\n}\n\nexport interface Transcription {\n  text: string;\n}\n\nexport interface TranscriptionCreateParams {\n  /**\n   * The audio file object (not file name) to transcribe, in one of these formats:\n   * flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Uploadable;\n\n  /**\n   * ID of the model to use. Only `whisper-1` is currently available.\n   */\n  model: (string & {}) | 'whisper-1';\n\n  /**\n   * The language of the input audio. Supplying the input language in\n   * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will\n   * improve accuracy and latency.\n   */\n  language?: string;\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio\n   * segment. The [prompt](/docs/guides/speech-to-text/prompting) should match the\n   * audio language.\n   */\n  prompt?: string;\n\n  /**\n   * The format of the transcript output, in one of these options: json, text, srt,\n   * verbose_json, or vtt.\n   */\n  response_format?: 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt';\n\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\n   * output more random, while lower values like 0.2 will make it more focused and\n   * deterministic. If set to 0, the model will use\n   * [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n}\n\nexport namespace Transcriptions {\n  export import Transcription = API.Transcription;\n  export import TranscriptionCreateParams = API.TranscriptionCreateParams;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../../core';\nimport { APIResource } from '../../resource';\nimport * as API from './index';\nimport { type Uploadable, multipartFormRequestOptions } from '../../core';\n\nexport class Translations extends APIResource {\n  /**\n   * Translates audio into English.\n   */\n  create(body: TranslationCreateParams, options?: Core.RequestOptions): Core.APIPromise<Translation> {\n    return this.post('/audio/translations', multipartFormRequestOptions({ body, ...options }));\n  }\n}\n\nexport interface Translation {\n  text: string;\n}\n\nexport interface TranslationCreateParams {\n  /**\n   * The audio file object (not file name) translate, in one of these formats: flac,\n   * mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Uploadable;\n\n  /**\n   * ID of the model to use. Only `whisper-1` is currently available.\n   */\n  model: (string & {}) | 'whisper-1';\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio\n   * segment. The [prompt](/docs/guides/speech-to-text/prompting) should be in\n   * English.\n   */\n  prompt?: string;\n\n  /**\n   * The format of the transcript output, in one of these options: json, text, srt,\n   * verbose_json, or vtt.\n   */\n  response_format?: string;\n\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\n   * output more random, while lower values like 0.2 will make it more focused and\n   * deterministic. If set to 0, the model will use\n   * [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n}\n\nexport namespace Translations {\n  export import Translation = API.Translation;\n  export import TranslationCreateParams = API.TranslationCreateParams;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport { APIResource } from '../../resource';\nimport { Transcriptions } from './transcriptions';\nimport { Translations } from './translations';\nimport * as API from './index';\n\nexport class Audio extends APIResource {\n  transcriptions: Transcriptions = new Transcriptions(this.client);\n  translations: Translations = new Translations(this.client);\n}\n\nexport namespace Audio {\n  export import Transcriptions = API.Transcriptions;\n  export import Transcription = API.Transcription;\n  export import TranscriptionCreateParams = API.TranscriptionCreateParams;\n\n  export import Translations = API.Translations;\n  export import Translation = API.Translation;\n  export import TranslationCreateParams = API.TranslationCreateParams;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../../core';\nimport { APIPromise } from '../../core';\nimport { APIResource } from '../../resource';\nimport * as Completions_ from '../completions';\nimport * as API from './index';\nimport { Stream } from '../../streaming';\n\nexport class Completions extends APIResource {\n  /**\n   * Creates a model response for the given chat conversation.\n   */\n  create(body: CompletionCreateParamsNonStreaming, options?: Core.RequestOptions): APIPromise<ChatCompletion>;\n  create(\n    body: CompletionCreateParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk>>;\n  create(\n    body: CompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk> | ChatCompletion>;\n  create(\n    body: CompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<ChatCompletion> | APIPromise<Stream<ChatCompletionChunk>> {\n    return this.post('/chat/completions', { body, ...options, stream: body.stream ?? false }) as\n      | APIPromise<ChatCompletion>\n      | APIPromise<Stream<ChatCompletionChunk>>;\n  }\n}\n\n/**\n * Represents a chat completion response returned by model, based on the provided\n * input.\n */\nexport interface ChatCompletion {\n  /**\n   * A unique identifier for the chat completion.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletion.Choice>;\n\n  /**\n   * A unix timestamp of when the chat completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for the chat completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion`.\n   */\n  object: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: Completions_.CompletionUsage;\n}\n\nexport namespace ChatCompletion {\n  export interface Choice {\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, or `function_call` if the\n     * model called a function.\n     */\n    finish_reason: 'stop' | 'length' | 'function_call';\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n\n    /**\n     * A chat completion message generated by the model.\n     */\n    message: ChatCompletionMessage;\n  }\n}\n\n/**\n * Represents a streamed chunk of a chat completion response returned by model,\n * based on the provided input.\n */\nexport interface ChatCompletionChunk {\n  /**\n   * A unique identifier for the chat completion chunk.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletionChunk.Choice>;\n\n  /**\n   * A unix timestamp of when the chat completion chunk was created.\n   */\n  created: number;\n\n  /**\n   * The model to generate the completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion.chunk`.\n   */\n  object: string;\n}\n\nexport namespace ChatCompletionChunk {\n  export interface Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    delta: Choice.Delta;\n\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, or `function_call` if the\n     * model called a function.\n     */\n    finish_reason: 'stop' | 'length' | 'function_call' | null;\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n  }\n\n  export namespace Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    export interface Delta {\n      /**\n       * The contents of the chunk message.\n       */\n      content?: string | null;\n\n      /**\n       * The name and arguments of a function that should be called, as generated by the\n       * model.\n       */\n      function_call?: Delta.FunctionCall;\n\n      /**\n       * The role of the author of this message.\n       */\n      role?: 'system' | 'user' | 'assistant' | 'function';\n    }\n\n    export namespace Delta {\n      /**\n       * The name and arguments of a function that should be called, as generated by the\n       * model.\n       */\n      export interface FunctionCall {\n        /**\n         * The arguments to call the function with, as generated by the model in JSON\n         * format. Note that the model does not always generate valid JSON, and may\n         * hallucinate parameters not defined by your function schema. Validate the\n         * arguments in your code before calling your function.\n         */\n        arguments?: string;\n\n        /**\n         * The name of the function to call.\n         */\n        name?: string;\n      }\n    }\n  }\n}\n\n/**\n * A chat completion message generated by the model.\n */\nexport interface ChatCompletionMessage {\n  /**\n   * The contents of the message.\n   */\n  content: string | null;\n\n  /**\n   * The role of the author of this message.\n   */\n  role: 'system' | 'user' | 'assistant' | 'function';\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  function_call?: ChatCompletionMessage.FunctionCall;\n}\n\nexport namespace ChatCompletionMessage {\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\nexport interface CreateChatCompletionRequestMessage {\n  /**\n   * The contents of the message. `content` is required for all messages, and may be\n   * null for assistant messages with function calls.\n   */\n  content: string | null;\n\n  /**\n   * The role of the messages author. One of `system`, `user`, `assistant`, or\n   * `function`.\n   */\n  role: 'system' | 'user' | 'assistant' | 'function';\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  function_call?: CreateChatCompletionRequestMessage.FunctionCall;\n\n  /**\n   * The name of the author of this message. `name` is required if role is\n   * `function`, and it should be the name of the function whose response is in the\n   * `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of\n   * 64 characters.\n   */\n  name?: string;\n}\n\nexport namespace CreateChatCompletionRequestMessage {\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\nexport interface CompletionCreateParams {\n  /**\n   * A list of messages comprising the conversation so far.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).\n   */\n  messages: Array<CreateChatCompletionRequestMessage>;\n\n  /**\n   * ID of the model to use. See the\n   * [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table\n   * for details on which models work with the Chat API.\n   */\n  model:\n    | (string & {})\n    | 'gpt-4'\n    | 'gpt-4-0314'\n    | 'gpt-4-0613'\n    | 'gpt-4-32k'\n    | 'gpt-4-32k-0314'\n    | 'gpt-4-32k-0613'\n    | 'gpt-3.5-turbo'\n    | 'gpt-3.5-turbo-16k'\n    | 'gpt-3.5-turbo-0301'\n    | 'gpt-3.5-turbo-0613'\n    | 'gpt-3.5-turbo-16k-0613';\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n   * existing frequency in the text so far, decreasing the model's likelihood to\n   * repeat the same line verbatim.\n   *\n   * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n   */\n  frequency_penalty?: number | null;\n\n  /**\n   * Controls how the model responds to function calls. \"none\" means the model does\n   * not call a function, and responds to the end-user. \"auto\" means the model can\n   * pick between an end-user or calling a function. Specifying a particular function\n   * via `{\"name\":\\ \"my_function\"}` forces the model to call that function. \"none\" is\n   * the default when no functions are present. \"auto\" is the default if functions\n   * are present.\n   */\n  function_call?: 'none' | 'auto' | CompletionCreateParams.FunctionCallOption;\n\n  /**\n   * A list of functions the model may generate JSON inputs for.\n   */\n  functions?: Array<CompletionCreateParams.Function>;\n\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a json object that maps tokens (specified by their token ID in the\n   * tokenizer) to an associated bias value from -100 to 100. Mathematically, the\n   * bias is added to the logits generated by the model prior to sampling. The exact\n   * effect will vary per model, but values between -1 and 1 should decrease or\n   * increase likelihood of selection; values like -100 or 100 should result in a ban\n   * or exclusive selection of the relevant token.\n   */\n  logit_bias?: Record<string, number> | null;\n\n  /**\n   * The maximum number of [tokens](/tokenizer) to generate in the chat completion.\n   *\n   * The total length of input tokens and generated tokens is limited by the model's\n   * context length.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n   * for counting tokens.\n   */\n  max_tokens?: number;\n\n  /**\n   * How many chat completion choices to generate for each input message.\n   */\n  n?: number | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on\n   * whether they appear in the text so far, increasing the model's likelihood to\n   * talk about new topics.\n   *\n   * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n   */\n  presence_penalty?: number | null;\n\n  /**\n   * Up to 4 sequences where the API will stop generating further tokens.\n   */\n  stop?: string | null | Array<string>;\n\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream?: boolean | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace CompletionCreateParams {\n  export interface FunctionCallOption {\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n\n  export interface Function {\n    /**\n     * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain\n     * underscores and dashes, with a maximum length of 64.\n     */\n    name: string;\n\n    /**\n     * The parameters the functions accepts, described as a JSON Schema object. See the\n     * [guide](/docs/guides/gpt/function-calling) for examples, and the\n     * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for\n     * documentation about the format.\n     *\n     * To describe a function that accepts no parameters, provide the value\n     * `{\"type\": \"object\", \"properties\": {}}`.\n     */\n    parameters: Record<string, unknown>;\n\n    /**\n     * A description of what the function does, used by the model to choose when and\n     * how to call the function.\n     */\n    description?: string;\n  }\n\n  export type CompletionCreateParamsNonStreaming = API.CompletionCreateParamsNonStreaming;\n  export type CompletionCreateParamsStreaming = API.CompletionCreateParamsStreaming;\n}\n\nexport interface CompletionCreateParamsNonStreaming extends CompletionCreateParams {\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream?: false | null;\n}\n\nexport interface CompletionCreateParamsStreaming extends CompletionCreateParams {\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream: true;\n}\n\nexport namespace Completions {\n  export import ChatCompletion = API.ChatCompletion;\n  export import ChatCompletionChunk = API.ChatCompletionChunk;\n  export import ChatCompletionMessage = API.ChatCompletionMessage;\n  export import CreateChatCompletionRequestMessage = API.CreateChatCompletionRequestMessage;\n  export import CompletionCreateParams = API.CompletionCreateParams;\n  export import CompletionCreateParamsNonStreaming = API.CompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsStreaming = API.CompletionCreateParamsStreaming;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport { APIResource } from '../../resource';\nimport { Completions } from './completions';\nimport * as API from './index';\n\nexport class Chat extends APIResource {\n  completions: Completions = new Completions(this.client);\n}\n\nexport namespace Chat {\n  export import Completions = API.Completions;\n  export import ChatCompletion = API.ChatCompletion;\n  export import ChatCompletionChunk = API.ChatCompletionChunk;\n  export import ChatCompletionMessage = API.ChatCompletionMessage;\n  export import CreateChatCompletionRequestMessage = API.CreateChatCompletionRequestMessage;\n  export import CompletionCreateParams = API.CompletionCreateParams;\n  export import CompletionCreateParamsNonStreaming = API.CompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsStreaming = API.CompletionCreateParamsStreaming;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../core';\nimport { APIPromise } from '../core';\nimport { APIResource } from '../resource';\nimport * as API from './index';\nimport { Stream } from '../streaming';\n\nexport class Completions extends APIResource {\n  /**\n   * Creates a completion for the provided prompt and parameters.\n   */\n  create(body: CompletionCreateParamsNonStreaming, options?: Core.RequestOptions): APIPromise<Completion>;\n  create(\n    body: CompletionCreateParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<Completion>>;\n  create(\n    body: CompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<Completion> | Completion>;\n  create(\n    body: CompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<Completion> | APIPromise<Stream<Completion>> {\n    return this.post('/completions', { body, ...options, stream: body.stream ?? false }) as\n      | APIPromise<Completion>\n      | APIPromise<Stream<Completion>>;\n  }\n}\n\n/**\n * Represents a completion response from the API. Note: both the streamed and\n * non-streamed response objects share the same shape (unlike the chat endpoint).\n */\nexport interface Completion {\n  /**\n   * A unique identifier for the completion.\n   */\n  id: string;\n\n  /**\n   * The list of completion choices the model generated for the input prompt.\n   */\n  choices: Array<CompletionChoice>;\n\n  /**\n   * The Unix timestamp of when the completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"text_completion\"\n   */\n  object: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: CompletionUsage;\n}\n\nexport interface CompletionChoice {\n  /**\n   * The reason the model stopped generating tokens. This will be `stop` if the model\n   * hit a natural stop point or a provided stop sequence, or `length` if the maximum\n   * number of tokens specified in the request was reached.\n   */\n  finish_reason: 'stop' | 'length';\n\n  index: number;\n\n  logprobs: CompletionChoice.Logprobs | null;\n\n  text: string;\n}\n\nexport namespace CompletionChoice {\n  export interface Logprobs {\n    text_offset?: Array<number>;\n\n    token_logprobs?: Array<number>;\n\n    tokens?: Array<string>;\n\n    top_logprobs?: Array<Record<string, number>>;\n  }\n}\n\n/**\n * Usage statistics for the completion request.\n */\nexport interface CompletionUsage {\n  /**\n   * Number of tokens in the generated completion.\n   */\n  completion_tokens: number;\n\n  /**\n   * Number of tokens in the prompt.\n   */\n  prompt_tokens: number;\n\n  /**\n   * Total number of tokens used in the request (prompt + completion).\n   */\n  total_tokens: number;\n}\n\nexport interface CompletionCreateParams {\n  /**\n   * ID of the model to use. You can use the\n   * [List models](/docs/api-reference/models/list) API to see all of your available\n   * models, or see our [Model overview](/docs/models/overview) for descriptions of\n   * them.\n   */\n  model:\n    | (string & {})\n    | 'text-davinci-003'\n    | 'text-davinci-002'\n    | 'text-davinci-001'\n    | 'code-davinci-002'\n    | 'text-curie-001'\n    | 'text-babbage-001'\n    | 'text-ada-001';\n\n  /**\n   * The prompt(s) to generate completions for, encoded as a string, array of\n   * strings, array of tokens, or array of token arrays.\n   *\n   * Note that <|endoftext|> is the document separator that the model sees during\n   * training, so if a prompt is not specified the model will generate as if from the\n   * beginning of a new document.\n   */\n  prompt: string | Array<string> | Array<number> | Array<Array<number>> | null;\n\n  /**\n   * Generates `best_of` completions server-side and returns the \"best\" (the one with\n   * the highest log probability per token). Results cannot be streamed.\n   *\n   * When used with `n`, `best_of` controls the number of candidate completions and\n   * `n` specifies how many to return  `best_of` must be greater than `n`.\n   *\n   * **Note:** Because this parameter generates many completions, it can quickly\n   * consume your token quota. Use carefully and ensure that you have reasonable\n   * settings for `max_tokens` and `stop`.\n   */\n  best_of?: number | null;\n\n  /**\n   * Echo back the prompt in addition to the completion\n   */\n  echo?: boolean | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n   * existing frequency in the text so far, decreasing the model's likelihood to\n   * repeat the same line verbatim.\n   *\n   * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n   */\n  frequency_penalty?: number | null;\n\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a json object that maps tokens (specified by their token ID in the GPT\n   * tokenizer) to an associated bias value from -100 to 100. You can use this\n   * [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to\n   * convert text to token IDs. Mathematically, the bias is added to the logits\n   * generated by the model prior to sampling. The exact effect will vary per model,\n   * but values between -1 and 1 should decrease or increase likelihood of selection;\n   * values like -100 or 100 should result in a ban or exclusive selection of the\n   * relevant token.\n   *\n   * As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token\n   * from being generated.\n   */\n  logit_bias?: Record<string, number> | null;\n\n  /**\n   * Include the log probabilities on the `logprobs` most likely tokens, as well the\n   * chosen tokens. For example, if `logprobs` is 5, the API will return a list of\n   * the 5 most likely tokens. The API will always return the `logprob` of the\n   * sampled token, so there may be up to `logprobs+1` elements in the response.\n   *\n   * The maximum value for `logprobs` is 5.\n   */\n  logprobs?: number | null;\n\n  /**\n   * The maximum number of [tokens](/tokenizer) to generate in the completion.\n   *\n   * The token count of your prompt plus `max_tokens` cannot exceed the model's\n   * context length.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n   * for counting tokens.\n   */\n  max_tokens?: number | null;\n\n  /**\n   * How many completions to generate for each prompt.\n   *\n   * **Note:** Because this parameter generates many completions, it can quickly\n   * consume your token quota. Use carefully and ensure that you have reasonable\n   * settings for `max_tokens` and `stop`.\n   */\n  n?: number | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on\n   * whether they appear in the text so far, increasing the model's likelihood to\n   * talk about new topics.\n   *\n   * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n   */\n  presence_penalty?: number | null;\n\n  /**\n   * Up to 4 sequences where the API will stop generating further tokens. The\n   * returned text will not contain the stop sequence.\n   */\n  stop?: string | null | Array<string>;\n\n  /**\n   * Whether to stream back partial progress. If set, tokens will be sent as\n   * data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream?: boolean | null;\n\n  /**\n   * The suffix that comes after a completion of inserted text.\n   */\n  suffix?: string | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace CompletionCreateParams {\n  export type CompletionCreateParamsNonStreaming = API.CompletionCreateParamsNonStreaming;\n  export type CompletionCreateParamsStreaming = API.CompletionCreateParamsStreaming;\n}\n\nexport interface CompletionCreateParamsNonStreaming extends CompletionCreateParams {\n  /**\n   * Whether to stream back partial progress. If set, tokens will be sent as\n   * data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream?: false | null;\n}\n\nexport interface CompletionCreateParamsStreaming extends CompletionCreateParams {\n  /**\n   * Whether to stream back partial progress. If set, tokens will be sent as\n   * data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream: true;\n}\n\nexport namespace Completions {\n  export import Completion = API.Completion;\n  export import CompletionChoice = API.CompletionChoice;\n  export import CompletionUsage = API.CompletionUsage;\n  export import CompletionCreateParams = API.CompletionCreateParams;\n  export import CompletionCreateParamsNonStreaming = API.CompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsStreaming = API.CompletionCreateParamsStreaming;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../core';\nimport { APIResource } from '../resource';\nimport * as API from './index';\n\nexport class Embeddings extends APIResource {\n  /**\n   * Creates an embedding vector representing the input text.\n   */\n  create(\n    body: EmbeddingCreateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<CreateEmbeddingResponse> {\n    return this.post('/embeddings', { body, ...options });\n  }\n}\n\nexport interface CreateEmbeddingResponse {\n  /**\n   * The list of embeddings generated by the model.\n   */\n  data: Array<Embedding>;\n\n  /**\n   * The name of the model used to generate the embedding.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"embedding\".\n   */\n  object: string;\n\n  /**\n   * The usage information for the request.\n   */\n  usage: CreateEmbeddingResponse.Usage;\n}\n\nexport namespace CreateEmbeddingResponse {\n  /**\n   * The usage information for the request.\n   */\n  export interface Usage {\n    /**\n     * The number of tokens used by the prompt.\n     */\n    prompt_tokens: number;\n\n    /**\n     * The total number of tokens used by the request.\n     */\n    total_tokens: number;\n  }\n}\n\n/**\n * Represents an embedding vector returned by embedding endpoint.\n */\nexport interface Embedding {\n  /**\n   * The embedding vector, which is a list of floats. The length of vector depends on\n   * the model as listed in the [embedding guide](/docs/guides/embeddings).\n   */\n  embedding: Array<number>;\n\n  /**\n   * The index of the embedding in the list of embeddings.\n   */\n  index: number;\n\n  /**\n   * The object type, which is always \"embedding\".\n   */\n  object: string;\n}\n\nexport interface EmbeddingCreateParams {\n  /**\n   * Input text to embed, encoded as a string or array of tokens. To embed multiple\n   * inputs in a single request, pass an array of strings or array of token arrays.\n   * Each input must not exceed the max input tokens for the model (8191 tokens for\n   * `text-embedding-ada-002`).\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n   * for counting tokens.\n   */\n  input: string | Array<string> | Array<number> | Array<Array<number>>;\n\n  /**\n   * ID of the model to use. You can use the\n   * [List models](/docs/api-reference/models/list) API to see all of your available\n   * models, or see our [Model overview](/docs/models/overview) for descriptions of\n   * them.\n   */\n  model: (string & {}) | 'text-embedding-ada-002';\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace Embeddings {\n  export import CreateEmbeddingResponse = API.CreateEmbeddingResponse;\n  export import Embedding = API.Embedding;\n  export import EmbeddingCreateParams = API.EmbeddingCreateParams;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../core';\nimport { APIResource } from '../resource';\nimport * as Completions from './completions';\nimport * as API from './index';\n\nexport class Edits extends APIResource {\n  /**\n   * Creates a new edit for the provided input, instruction, and parameters.\n   *\n   * @deprecated The Edits API is deprecated; please use Chat Completions instead.\n   *\n   * https://openai.com/blog/gpt-4-api-general-availability#deprecation-of-the-edits-api\n   */\n  create(body: EditCreateParams, options?: Core.RequestOptions): Core.APIPromise<Edit> {\n    return this.post('/edits', { body, ...options });\n  }\n}\n\nexport interface Edit {\n  /**\n   * A list of edit choices. Can be more than one if `n` is greater than 1.\n   */\n  choices: Array<Edit.Choice>;\n\n  /**\n   * A unix timestamp of when the edit was created.\n   */\n  created: number;\n\n  /**\n   * The object type, which is always `edit`.\n   */\n  object: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage: Completions.CompletionUsage;\n}\n\nexport namespace Edit {\n  export interface Choice {\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, or `length` if the maximum\n     * number of tokens specified in the request was reached.\n     */\n    finish_reason: 'stop' | 'length';\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n\n    /**\n     * The edited result.\n     */\n    text: string;\n  }\n}\n\nexport interface EditCreateParams {\n  /**\n   * The instruction that tells the model how to edit the prompt.\n   */\n  instruction: string;\n\n  /**\n   * ID of the model to use. You can use the `text-davinci-edit-001` or\n   * `code-davinci-edit-001` model with this endpoint.\n   */\n  model: (string & {}) | 'text-davinci-edit-001' | 'code-davinci-edit-001';\n\n  /**\n   * The input text to use as a starting point for the edit.\n   */\n  input?: string | null;\n\n  /**\n   * How many edits to generate for the input and instruction.\n   */\n  n?: number | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n}\n\nexport namespace Edits {\n  export import Edit = API.Edit;\n  export import EditCreateParams = API.EditCreateParams;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../core';\nimport { APIResource } from '../resource';\nimport * as API from './index';\nimport { type Uploadable, multipartFormRequestOptions } from '../core';\nimport { Page } from '../pagination';\n\nexport class Files extends APIResource {\n  /**\n   * Upload a file that contains document(s) to be used across various\n   * endpoints/features. Currently, the size of all the files uploaded by one\n   * organization can be up to 1 GB. Please contact us if you need to increase the\n   * storage limit.\n   */\n  create(body: FileCreateParams, options?: Core.RequestOptions): Core.APIPromise<FileObject> {\n    return this.post('/files', multipartFormRequestOptions({ body, ...options }));\n  }\n\n  /**\n   * Returns information about a specific file.\n   */\n  retrieve(fileId: string, options?: Core.RequestOptions): Core.APIPromise<FileObject> {\n    return this.get(`/files/${fileId}`, options);\n  }\n\n  /**\n   * Returns a list of files that belong to the user's organization.\n   */\n  list(options?: Core.RequestOptions): Core.PagePromise<FileObjectsPage, FileObject> {\n    return this.getAPIList('/files', FileObjectsPage, options);\n  }\n\n  /**\n   * Delete a file.\n   */\n  del(fileId: string, options?: Core.RequestOptions): Core.APIPromise<FileDeleted> {\n    return this.delete(`/files/${fileId}`, options);\n  }\n\n  /**\n   * Returns the contents of the specified file\n   */\n  retrieveContent(fileId: string, options?: Core.RequestOptions): Core.APIPromise<string> {\n    return this.get(`/files/${fileId}/content`, {\n      ...options,\n      headers: { Accept: 'application/json', ...options?.headers },\n    });\n  }\n}\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class FileObjectsPage extends Page<FileObject> {}\n// alias so we can export it in the namespace\ntype _FileObjectsPage = FileObjectsPage;\n\nexport type FileContent = string;\n\nexport interface FileDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: string;\n}\n\n/**\n * The `File` object represents a document that has been uploaded to OpenAI.\n */\nexport interface FileObject {\n  /**\n   * The file identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The size of the file in bytes.\n   */\n  bytes: number;\n\n  /**\n   * The unix timestamp for when the file was created.\n   */\n  created_at: number;\n\n  /**\n   * The name of the file.\n   */\n  filename: string;\n\n  /**\n   * The object type, which is always \"file\".\n   */\n  object: string;\n\n  /**\n   * The intended purpose of the file. Currently, only \"fine-tune\" is supported.\n   */\n  purpose: string;\n\n  /**\n   * The current status of the file, which can be either `uploaded`, `processed`,\n   * `pending`, `error`, `deleting` or `deleted`.\n   */\n  status?: string;\n\n  /**\n   * Additional details about the status of the file. If the file is in the `error`\n   * state, this will include a message describing the error.\n   */\n  status_details?: string | null;\n}\n\nexport interface FileCreateParams {\n  /**\n   * Name of the [JSON Lines](https://jsonlines.readthedocs.io/en/latest/) file to be\n   * uploaded.\n   *\n   * If the `purpose` is set to \"fine-tune\", each line is a JSON record with \"prompt\"\n   * and \"completion\" fields representing your\n   * [training examples](/docs/guides/fine-tuning/prepare-training-data).\n   */\n  file: Uploadable;\n\n  /**\n   * The intended purpose of the uploaded documents.\n   *\n   * Use \"fine-tune\" for [Fine-tuning](/docs/api-reference/fine-tunes). This allows\n   * us to validate the format of the uploaded file.\n   */\n  purpose: string;\n}\n\nexport namespace Files {\n  export import FileContent = API.FileContent;\n  export import FileDeleted = API.FileDeleted;\n  export import FileObject = API.FileObject;\n  export type FileObjectsPage = _FileObjectsPage;\n  export import FileCreateParams = API.FileCreateParams;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../core';\nimport { APIPromise } from '../core';\nimport { APIResource } from '../resource';\nimport * as Files from './files';\nimport * as API from './index';\nimport { Page } from '../pagination';\nimport { Stream } from '../streaming';\n\nexport class FineTunes extends APIResource {\n  /**\n   * Creates a job that fine-tunes a specified model from a given dataset.\n   *\n   * Response includes details of the enqueued job including job status and the name\n   * of the fine-tuned models once complete.\n   *\n   * [Learn more about Fine-tuning](/docs/guides/fine-tuning)\n   */\n  create(body: FineTuneCreateParams, options?: Core.RequestOptions): Core.APIPromise<FineTune> {\n    return this.post('/fine-tunes', { body, ...options });\n  }\n\n  /**\n   * Gets info about the fine-tune job.\n   *\n   * [Learn more about Fine-tuning](/docs/guides/fine-tuning)\n   */\n  retrieve(fineTuneId: string, options?: Core.RequestOptions): Core.APIPromise<FineTune> {\n    return this.get(`/fine-tunes/${fineTuneId}`, options);\n  }\n\n  /**\n   * List your organization's fine-tuning jobs\n   */\n  list(options?: Core.RequestOptions): Core.PagePromise<FineTunesPage, FineTune> {\n    return this.getAPIList('/fine-tunes', FineTunesPage, options);\n  }\n\n  /**\n   * Immediately cancel a fine-tune job.\n   */\n  cancel(fineTuneId: string, options?: Core.RequestOptions): Core.APIPromise<FineTune> {\n    return this.post(`/fine-tunes/${fineTuneId}/cancel`, options);\n  }\n\n  /**\n   * Get fine-grained status updates for a fine-tune job.\n   */\n  listEvents(\n    fineTuneId: string,\n    query?: FineTuneListEventsParamsNonStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<FineTuneEventsListResponse>;\n  listEvents(\n    fineTuneId: string,\n    query: FineTuneListEventsParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<FineTuneEvent>>;\n  listEvents(\n    fineTuneId: string,\n    query?: FineTuneListEventsParams | undefined,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<FineTuneEvent> | FineTuneEventsListResponse>;\n  listEvents(\n    fineTuneId: string,\n    query?: FineTuneListEventsParams | undefined,\n    options?: Core.RequestOptions,\n  ): APIPromise<FineTuneEventsListResponse> | APIPromise<Stream<FineTuneEvent>> {\n    return this.get(`/fine-tunes/${fineTuneId}/events`, {\n      query,\n      timeout: 86400000,\n      ...options,\n      stream: query?.stream ?? false,\n    }) as APIPromise<FineTuneEventsListResponse> | APIPromise<Stream<FineTuneEvent>>;\n  }\n}\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class FineTunesPage extends Page<FineTune> {}\n// alias so we can export it in the namespace\ntype _FineTunesPage = FineTunesPage;\n\n/**\n * The `FineTune` object represents a fine-tuning job that has been created through\n * the API.\n */\nexport interface FineTune {\n  /**\n   * The object identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The unix timestamp for when the fine-tuning job was created.\n   */\n  created_at: number;\n\n  /**\n   * The name of the fine-tuned model that is being created.\n   */\n  fine_tuned_model: string | null;\n\n  /**\n   * The hyperparameters used for the fine-tuning job. See the\n   * [Fine-tuning Guide](/docs/guides/fine-tuning/hyperparameters) for more details.\n   */\n  hyperparams: FineTune.Hyperparams;\n\n  /**\n   * The base model that is being fine-tuned.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"fine-tune\".\n   */\n  object: string;\n\n  /**\n   * The organization that owns the fine-tuning job.\n   */\n  organization_id: string;\n\n  /**\n   * The compiled results files for the fine-tuning job.\n   */\n  result_files: Array<Files.FileObject>;\n\n  /**\n   * The current status of the fine-tuning job, which can be either `created`,\n   * `pending`, `running`, `succeeded`, `failed`, or `cancelled`.\n   */\n  status: string;\n\n  /**\n   * The list of files used for training.\n   */\n  training_files: Array<Files.FileObject>;\n\n  /**\n   * The unix timestamp for when the fine-tuning job was last updated.\n   */\n  updated_at: number;\n\n  /**\n   * The list of files used for validation.\n   */\n  validation_files: Array<Files.FileObject>;\n\n  /**\n   * The list of events that have been observed in the lifecycle of the FineTune job.\n   */\n  events?: Array<FineTuneEvent>;\n}\n\nexport namespace FineTune {\n  /**\n   * The hyperparameters used for the fine-tuning job. See the\n   * [Fine-tuning Guide](/docs/guides/fine-tuning/hyperparameters) for more details.\n   */\n  export interface Hyperparams {\n    /**\n     * The batch size to use for training. The batch size is the number of training\n     * examples used to train a single forward and backward pass.\n     */\n    batch_size: number;\n\n    /**\n     * The learning rate multiplier to use for training.\n     */\n    learning_rate_multiplier: number;\n\n    /**\n     * The number of epochs to train the model for. An epoch refers to one full cycle\n     * through the training dataset.\n     */\n    n_epochs: number;\n\n    /**\n     * The weight to use for loss on the prompt tokens.\n     */\n    prompt_loss_weight: number;\n\n    /**\n     * The number of classes to use for computing classification metrics.\n     */\n    classification_n_classes?: number;\n\n    /**\n     * The positive class to use for computing classification metrics.\n     */\n    classification_positive_class?: string;\n\n    /**\n     * The classification metrics to compute using the validation dataset at the end of\n     * every epoch.\n     */\n    compute_classification_metrics?: boolean;\n  }\n}\n\nexport interface FineTuneEvent {\n  created_at: number;\n\n  level: string;\n\n  message: string;\n\n  object: string;\n}\n\nexport interface FineTuneEventsListResponse {\n  data: Array<FineTuneEvent>;\n\n  object: string;\n}\n\nexport interface FineTuneCreateParams {\n  /**\n   * The ID of an uploaded file that contains training data.\n   *\n   * See [upload file](/docs/api-reference/files/upload) for how to upload a file.\n   *\n   * Your dataset must be formatted as a JSONL file, where each training example is a\n   * JSON object with the keys \"prompt\" and \"completion\". Additionally, you must\n   * upload your file with the purpose `fine-tune`.\n   *\n   * See the [fine-tuning guide](/docs/guides/fine-tuning/creating-training-data) for\n   * more details.\n   */\n  training_file: string;\n\n  /**\n   * The batch size to use for training. The batch size is the number of training\n   * examples used to train a single forward and backward pass.\n   *\n   * By default, the batch size will be dynamically configured to be ~0.2% of the\n   * number of examples in the training set, capped at 256 - in general, we've found\n   * that larger batch sizes tend to work better for larger datasets.\n   */\n  batch_size?: number | null;\n\n  /**\n   * If this is provided, we calculate F-beta scores at the specified beta values.\n   * The F-beta score is a generalization of F-1 score. This is only used for binary\n   * classification.\n   *\n   * With a beta of 1 (i.e. the F-1 score), precision and recall are given the same\n   * weight. A larger beta score puts more weight on recall and less on precision. A\n   * smaller beta score puts more weight on precision and less on recall.\n   */\n  classification_betas?: Array<number> | null;\n\n  /**\n   * The number of classes in a classification task.\n   *\n   * This parameter is required for multiclass classification.\n   */\n  classification_n_classes?: number | null;\n\n  /**\n   * The positive class in binary classification.\n   *\n   * This parameter is needed to generate precision, recall, and F1 metrics when\n   * doing binary classification.\n   */\n  classification_positive_class?: string | null;\n\n  /**\n   * If set, we calculate classification-specific metrics such as accuracy and F-1\n   * score using the validation set at the end of every epoch. These metrics can be\n   * viewed in the\n   * [results file](/docs/guides/fine-tuning/analyzing-your-fine-tuned-model).\n   *\n   * In order to compute classification metrics, you must provide a\n   * `validation_file`. Additionally, you must specify `classification_n_classes` for\n   * multiclass classification or `classification_positive_class` for binary\n   * classification.\n   */\n  compute_classification_metrics?: boolean | null;\n\n  /**\n   * The learning rate multiplier to use for training. The fine-tuning learning rate\n   * is the original learning rate used for pretraining multiplied by this value.\n   *\n   * By default, the learning rate multiplier is the 0.05, 0.1, or 0.2 depending on\n   * final `batch_size` (larger learning rates tend to perform better with larger\n   * batch sizes). We recommend experimenting with values in the range 0.02 to 0.2 to\n   * see what produces the best results.\n   */\n  learning_rate_multiplier?: number | null;\n\n  /**\n   * The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\",\n   * \"curie\", \"davinci\", or a fine-tuned model created after 2022-04-21. To learn\n   * more about these models, see the\n   * [Models](https://platform.openai.com/docs/models) documentation.\n   */\n  model?: (string & {}) | 'ada' | 'babbage' | 'curie' | 'davinci' | null;\n\n  /**\n   * The number of epochs to train the model for. An epoch refers to one full cycle\n   * through the training dataset.\n   */\n  n_epochs?: number | null;\n\n  /**\n   * The weight to use for loss on the prompt tokens. This controls how much the\n   * model tries to learn to generate the prompt (as compared to the completion which\n   * always has a weight of 1.0), and can add a stabilizing effect to training when\n   * completions are short.\n   *\n   * If prompts are extremely long (relative to completions), it may make sense to\n   * reduce this weight so as to avoid over-prioritizing learning the prompt.\n   */\n  prompt_loss_weight?: number | null;\n\n  /**\n   * A string of up to 40 characters that will be added to your fine-tuned model\n   * name.\n   *\n   * For example, a `suffix` of \"custom-model-name\" would produce a model name like\n   * `ada:ft-your-org:custom-model-name-2022-02-15-04-21-04`.\n   */\n  suffix?: string | null;\n\n  /**\n   * The ID of an uploaded file that contains validation data.\n   *\n   * If you provide this file, the data is used to generate validation metrics\n   * periodically during fine-tuning. These metrics can be viewed in the\n   * [fine-tuning results file](/docs/guides/fine-tuning/analyzing-your-fine-tuned-model).\n   * Your train and validation data should be mutually exclusive.\n   *\n   * Your dataset must be formatted as a JSONL file, where each validation example is\n   * a JSON object with the keys \"prompt\" and \"completion\". Additionally, you must\n   * upload your file with the purpose `fine-tune`.\n   *\n   * See the [fine-tuning guide](/docs/guides/fine-tuning/creating-training-data) for\n   * more details.\n   */\n  validation_file?: string | null;\n}\n\nexport interface FineTuneListEventsParams {\n  /**\n   * Whether to stream events for the fine-tune job. If set to true, events will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available. The stream will terminate with a `data: [DONE]`\n   * message when the job is finished (succeeded, cancelled, or failed).\n   *\n   * If set to false, only events generated so far will be returned.\n   */\n  stream?: boolean;\n}\n\nexport namespace FineTuneListEventsParams {\n  export type FineTuneListEventsParamsNonStreaming = API.FineTuneListEventsParamsNonStreaming;\n  export type FineTuneListEventsParamsStreaming = API.FineTuneListEventsParamsStreaming;\n}\n\nexport interface FineTuneListEventsParamsNonStreaming extends FineTuneListEventsParams {\n  /**\n   * Whether to stream events for the fine-tune job. If set to true, events will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available. The stream will terminate with a `data: [DONE]`\n   * message when the job is finished (succeeded, cancelled, or failed).\n   *\n   * If set to false, only events generated so far will be returned.\n   */\n  stream?: false;\n}\n\nexport interface FineTuneListEventsParamsStreaming extends FineTuneListEventsParams {\n  /**\n   * Whether to stream events for the fine-tune job. If set to true, events will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available. The stream will terminate with a `data: [DONE]`\n   * message when the job is finished (succeeded, cancelled, or failed).\n   *\n   * If set to false, only events generated so far will be returned.\n   */\n  stream: true;\n}\n\nexport namespace FineTunes {\n  export import FineTune = API.FineTune;\n  export import FineTuneEvent = API.FineTuneEvent;\n  export import FineTuneEventsListResponse = API.FineTuneEventsListResponse;\n  export type FineTunesPage = _FineTunesPage;\n  export import FineTuneCreateParams = API.FineTuneCreateParams;\n  export import FineTuneListEventsParams = API.FineTuneListEventsParams;\n  export import FineTuneListEventsParamsNonStreaming = API.FineTuneListEventsParamsNonStreaming;\n  export import FineTuneListEventsParamsStreaming = API.FineTuneListEventsParamsStreaming;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../core';\nimport { APIResource } from '../resource';\nimport * as API from './index';\nimport { type Uploadable, multipartFormRequestOptions } from '../core';\n\nexport class Images extends APIResource {\n  /**\n   * Creates a variation of a given image.\n   */\n  createVariation(\n    body: ImageCreateVariationParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<ImagesResponse> {\n    return this.post('/images/variations', multipartFormRequestOptions({ body, ...options }));\n  }\n\n  /**\n   * Creates an edited or extended image given an original image and a prompt.\n   */\n  edit(body: ImageEditParams, options?: Core.RequestOptions): Core.APIPromise<ImagesResponse> {\n    return this.post('/images/edits', multipartFormRequestOptions({ body, ...options }));\n  }\n\n  /**\n   * Creates an image given a prompt.\n   */\n  generate(body: ImageGenerateParams, options?: Core.RequestOptions): Core.APIPromise<ImagesResponse> {\n    return this.post('/images/generations', { body, ...options });\n  }\n}\n\n/**\n * Represents the url or the content of an image generated by the OpenAI API.\n */\nexport interface Image {\n  /**\n   * The base64-encoded JSON of the generated image, if `response_format` is\n   * `b64_json`.\n   */\n  b64_json?: string;\n\n  /**\n   * The URL of the generated image, if `response_format` is `url` (default).\n   */\n  url?: string;\n}\n\nexport interface ImagesResponse {\n  created: number;\n\n  data: Array<Image>;\n}\n\nexport interface ImageCreateVariationParams {\n  /**\n   * The image to use as the basis for the variation(s). Must be a valid PNG file,\n   * less than 4MB, and square.\n   */\n  image: Uploadable;\n\n  /**\n   * The number of images to generate. Must be between 1 and 10.\n   */\n  n?: number | null;\n\n  /**\n   * The format in which the generated images are returned. Must be one of `url` or\n   * `b64_json`.\n   */\n  response_format?: 'url' | 'b64_json' | null;\n\n  /**\n   * The size of the generated images. Must be one of `256x256`, `512x512`, or\n   * `1024x1024`.\n   */\n  size?: '256x256' | '512x512' | '1024x1024' | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport interface ImageEditParams {\n  /**\n   * The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask\n   * is not provided, image must have transparency, which will be used as the mask.\n   */\n  image: Uploadable;\n\n  /**\n   * A text description of the desired image(s). The maximum length is 1000\n   * characters.\n   */\n  prompt: string;\n\n  /**\n   * An additional image whose fully transparent areas (e.g. where alpha is zero)\n   * indicate where `image` should be edited. Must be a valid PNG file, less than\n   * 4MB, and have the same dimensions as `image`.\n   */\n  mask?: Uploadable;\n\n  /**\n   * The number of images to generate. Must be between 1 and 10.\n   */\n  n?: number | null;\n\n  /**\n   * The format in which the generated images are returned. Must be one of `url` or\n   * `b64_json`.\n   */\n  response_format?: 'url' | 'b64_json' | null;\n\n  /**\n   * The size of the generated images. Must be one of `256x256`, `512x512`, or\n   * `1024x1024`.\n   */\n  size?: '256x256' | '512x512' | '1024x1024' | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport interface ImageGenerateParams {\n  /**\n   * A text description of the desired image(s). The maximum length is 1000\n   * characters.\n   */\n  prompt: string;\n\n  /**\n   * The number of images to generate. Must be between 1 and 10.\n   */\n  n?: number | null;\n\n  /**\n   * The format in which the generated images are returned. Must be one of `url` or\n   * `b64_json`.\n   */\n  response_format?: 'url' | 'b64_json' | null;\n\n  /**\n   * The size of the generated images. Must be one of `256x256`, `512x512`, or\n   * `1024x1024`.\n   */\n  size?: '256x256' | '512x512' | '1024x1024' | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace Images {\n  export import Image = API.Image;\n  export import ImagesResponse = API.ImagesResponse;\n  export import ImageCreateVariationParams = API.ImageCreateVariationParams;\n  export import ImageEditParams = API.ImageEditParams;\n  export import ImageGenerateParams = API.ImageGenerateParams;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../core';\nimport { APIResource } from '../resource';\nimport * as API from './index';\nimport { Page } from '../pagination';\n\nexport class Models extends APIResource {\n  /**\n   * Retrieves a model instance, providing basic information about the model such as\n   * the owner and permissioning.\n   */\n  retrieve(model: string, options?: Core.RequestOptions): Core.APIPromise<Model> {\n    return this.get(`/models/${model}`, options);\n  }\n\n  /**\n   * Lists the currently available models, and provides basic information about each\n   * one such as the owner and availability.\n   */\n  list(options?: Core.RequestOptions): Core.PagePromise<ModelsPage, Model> {\n    return this.getAPIList('/models', ModelsPage, options);\n  }\n\n  /**\n   * Delete a fine-tuned model. You must have the Owner role in your organization.\n   */\n  del(model: string, options?: Core.RequestOptions): Core.APIPromise<ModelDeleted> {\n    return this.delete(`/models/${model}`, options);\n  }\n}\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class ModelsPage extends Page<Model> {}\n// alias so we can export it in the namespace\ntype _ModelsPage = ModelsPage;\n\n/**\n * Describes an OpenAI model offering that can be used with the API.\n */\nexport interface Model {\n  /**\n   * The model identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The date and time when the model was created.\n   */\n  created: number;\n\n  /**\n   * The object type, which is always \"model\".\n   */\n  object: string;\n\n  /**\n   * The organization that owns the model.\n   */\n  owned_by: string;\n}\n\nexport interface ModelDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: string;\n}\n\nexport namespace Models {\n  export import Model = API.Model;\n  export import ModelDeleted = API.ModelDeleted;\n  export type ModelsPage = _ModelsPage;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../core';\nimport { APIResource } from '../resource';\nimport * as API from './index';\n\nexport class Moderations extends APIResource {\n  /**\n   * Classifies if text violates OpenAI's Content Policy\n   */\n  create(\n    body: ModerationCreateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<ModerationCreateResponse> {\n    return this.post('/moderations', { body, ...options });\n  }\n}\n\nexport interface Moderation {\n  /**\n   * A list of the categories, and whether they are flagged or not.\n   */\n  categories: Moderation.Categories;\n\n  /**\n   * A list of the categories along with their scores as predicted by model.\n   */\n  category_scores: Moderation.CategoryScores;\n\n  /**\n   * Whether the content violates\n   * [OpenAI's usage policies](/policies/usage-policies).\n   */\n  flagged: boolean;\n}\n\nexport namespace Moderation {\n  /**\n   * A list of the categories, and whether they are flagged or not.\n   */\n  export interface Categories {\n    /**\n     * Content that expresses, incites, or promotes harassing language towards any\n     * target.\n     */\n    harassment: boolean;\n\n    /**\n     * Harassment content that also includes violence or serious harm towards any\n     * target.\n     */\n    'harassment/threatening': boolean;\n\n    /**\n     * Content that expresses, incites, or promotes hate based on race, gender,\n     * ethnicity, religion, nationality, sexual orientation, disability status, or\n     * caste. Hateful content aimed at non-protected groups (e.g., chess players) is\n     * harrassment.\n     */\n    hate: boolean;\n\n    /**\n     * Hateful content that also includes violence or serious harm towards the targeted\n     * group based on race, gender, ethnicity, religion, nationality, sexual\n     * orientation, disability status, or caste.\n     */\n    'hate/threatening': boolean;\n\n    /**\n     * Content that promotes, encourages, or depicts acts of self-harm, such as\n     * suicide, cutting, and eating disorders.\n     */\n    'self-harm': boolean;\n\n    /**\n     * Content that encourages performing acts of self-harm, such as suicide, cutting,\n     * and eating disorders, or that gives instructions or advice on how to commit such\n     * acts.\n     */\n    'self-harm/instructions': boolean;\n\n    /**\n     * Content where the speaker expresses that they are engaging or intend to engage\n     * in acts of self-harm, such as suicide, cutting, and eating disorders.\n     */\n    'self-harm/intent': boolean;\n\n    /**\n     * Content meant to arouse sexual excitement, such as the description of sexual\n     * activity, or that promotes sexual services (excluding sex education and\n     * wellness).\n     */\n    sexual: boolean;\n\n    /**\n     * Sexual content that includes an individual who is under 18 years old.\n     */\n    'sexual/minors': boolean;\n\n    /**\n     * Content that depicts death, violence, or physical injury.\n     */\n    violence: boolean;\n\n    /**\n     * Content that depicts death, violence, or physical injury in graphic detail.\n     */\n    'violence/graphic': boolean;\n  }\n\n  /**\n   * A list of the categories along with their scores as predicted by model.\n   */\n  export interface CategoryScores {\n    /**\n     * The score for the category 'harassment'.\n     */\n    harassment: number;\n\n    /**\n     * The score for the category 'harassment/threatening'.\n     */\n    'harassment/threatening': number;\n\n    /**\n     * The score for the category 'hate'.\n     */\n    hate: number;\n\n    /**\n     * The score for the category 'hate/threatening'.\n     */\n    'hate/threatening': number;\n\n    /**\n     * The score for the category 'self-harm'.\n     */\n    'self-harm': number;\n\n    /**\n     * The score for the category 'self-harm/instructions'.\n     */\n    'self-harm/instructions': number;\n\n    /**\n     * The score for the category 'self-harm/intent'.\n     */\n    'self-harm/intent': number;\n\n    /**\n     * The score for the category 'sexual'.\n     */\n    sexual: number;\n\n    /**\n     * The score for the category 'sexual/minors'.\n     */\n    'sexual/minors': number;\n\n    /**\n     * The score for the category 'violence'.\n     */\n    violence: number;\n\n    /**\n     * The score for the category 'violence/graphic'.\n     */\n    'violence/graphic': number;\n  }\n}\n\n/**\n * Represents policy compliance report by OpenAI's content moderation model against\n * a given input.\n */\nexport interface ModerationCreateResponse {\n  /**\n   * The unique identifier for the moderation request.\n   */\n  id: string;\n\n  /**\n   * The model used to generate the moderation results.\n   */\n  model: string;\n\n  /**\n   * A list of moderation objects.\n   */\n  results: Array<Moderation>;\n}\n\nexport interface ModerationCreateParams {\n  /**\n   * The input text to classify\n   */\n  input: string | Array<string>;\n\n  /**\n   * Two content moderations models are available: `text-moderation-stable` and\n   * `text-moderation-latest`.\n   *\n   * The default is `text-moderation-latest` which will be automatically upgraded\n   * over time. This ensures you are always using our most accurate model. If you use\n   * `text-moderation-stable`, we will provide advanced notice before updating the\n   * model. Accuracy of `text-moderation-stable` may be slightly lower than for\n   * `text-moderation-latest`.\n   */\n  model?: (string & {}) | 'text-moderation-latest' | 'text-moderation-stable';\n}\n\nexport namespace Moderations {\n  export import Moderation = API.Moderation;\n  export import ModerationCreateResponse = API.ModerationCreateResponse;\n  export import ModerationCreateParams = API.ModerationCreateParams;\n}\n", "// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from './core';\nimport * as Pagination from './pagination';\nimport * as API from './resources/index';\nimport * as Errors from './error';\nimport type { Agent } from './_shims/agent';\nimport * as Uploads from './uploads';\n\nexport interface ClientOptions {\n  /**\n   * Defaults to process.env[\"OPENAI_API_KEY\"].\n   */\n  apiKey?: string;\n\n  /**\n   * Override the default base URL for the API, e.g., \"https://api.example.com/v2/\"\n   */\n  baseURL?: string;\n\n  /**\n   * The maximum amount of time (in milliseconds) that the client should wait for a response\n   * from the server before timing out a single request.\n   *\n   * Note that request timeouts are retried by default, so in a worst-case scenario you may wait\n   * much longer than this timeout before the promise succeeds or fails.\n   */\n  timeout?: number;\n\n  /**\n   * An HTTP agent used to manage HTTP(S) connections.\n   *\n   * If not provided, an agent will be constructed by default in the Node.js environment,\n   * otherwise no agent is used.\n   */\n  httpAgent?: Agent;\n\n  /**\n   * Specify a custom `fetch` function implementation.\n   *\n   * If not provided, we use `node-fetch` on Node.js and otherwise expect that `fetch` is\n   * defined globally.\n   */\n  fetch?: Core.Fetch | undefined;\n\n  /**\n   * The maximum number of times that the client will retry a request in case of a\n   * temporary failure, like a network error or a 5XX error from the server.\n   *\n   * @default 2\n   */\n  maxRetries?: number;\n\n  /**\n   * Default headers to include with every request to the API.\n   *\n   * These can be removed in individual requests by explicitly setting the\n   * header to `undefined` or `null` in request options.\n   */\n  defaultHeaders?: Core.Headers;\n\n  /**\n   * Default query parameters to include with every request to the API.\n   *\n   * These can be removed in individual requests by explicitly setting the\n   * param to `undefined` in request options.\n   */\n  defaultQuery?: Core.DefaultQuery;\n\n  /**\n   * By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.\n   * Only set this option to `true` if you understand the risks and have appropriate mitigations in place.\n   */\n  dangerouslyAllowBrowser?: boolean;\n\n  organization?: string | null;\n}\n\n/** API Client for interfacing with the OpenAI API. */\nexport class OpenAI extends Core.APIClient {\n  apiKey: string;\n  organization?: string | null;\n\n  private _options: ClientOptions;\n\n  /**\n   * API Client for interfacing with the OpenAI API.\n   *\n   * @param {string} [opts.apiKey=process.env['OPENAI_API_KEY']] - The API Key to send to the API.\n   * @param {string} [opts.baseURL] - Override the default base URL for the API.\n   * @param {number} [opts.timeout=10 minutes] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.\n   * @param {number} [opts.httpAgent] - An HTTP agent used to manage HTTP(s) connections.\n   * @param {Core.Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.\n   * @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.\n   * @param {Core.Headers} opts.defaultHeaders - Default headers to include with every request to the API.\n   * @param {Core.DefaultQuery} opts.defaultQuery - Default query parameters to include with every request to the API.\n   * @param {boolean} [opts.dangerouslyAllowBrowser=false] - By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.\n   * @param {string | null} [opts.organization]\n   */\n  constructor({\n    apiKey = Core.readEnv('OPENAI_API_KEY'),\n    organization = Core.readEnv('OPENAI_ORG_ID') ?? null,\n    ...opts\n  }: ClientOptions = {}) {\n    if (apiKey === undefined) {\n      throw new Error(\n        \"The OPENAI_API_KEY environment variable is missing or empty; either provide it, or instantiate the OpenAI client with an apiKey option, like new OpenAI({ apiKey: 'my apiKey' }).\",\n      );\n    }\n\n    const options: ClientOptions = {\n      apiKey,\n      organization,\n      baseURL: `https://api.openai.com/v1`,\n      ...opts,\n    };\n\n    if (!options.dangerouslyAllowBrowser && Core.isRunningInBrowser()) {\n      throw new Error(\n        \"It looks like you're running in a browser-like environment.\\n\\nThis is disabled by default, as it risks exposing your secret API credentials to attackers.\\nIf you understand the risks and have appropriate mitigations in place,\\nyou can set the `dangerouslyAllowBrowser` option to `true`, e.g.,\\n\\nnew OpenAI({ apiKey, dangerouslyAllowBrowser: true });\\n\\nhttps://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\\n\",\n      );\n    }\n\n    super({\n      baseURL: options.baseURL!,\n      timeout: options.timeout ?? 600000 /* 10 minutes */,\n      httpAgent: options.httpAgent,\n      maxRetries: options.maxRetries,\n      fetch: options.fetch,\n    });\n    this._options = options;\n\n    this.apiKey = apiKey;\n    this.organization = organization;\n  }\n\n  completions: API.Completions = new API.Completions(this);\n  chat: API.Chat = new API.Chat(this);\n  edits: API.Edits = new API.Edits(this);\n  embeddings: API.Embeddings = new API.Embeddings(this);\n  files: API.Files = new API.Files(this);\n  images: API.Images = new API.Images(this);\n  audio: API.Audio = new API.Audio(this);\n  moderations: API.Moderations = new API.Moderations(this);\n  models: API.Models = new API.Models(this);\n  fineTunes: API.FineTunes = new API.FineTunes(this);\n\n  protected override defaultQuery(): Core.DefaultQuery | undefined {\n    return this._options.defaultQuery;\n  }\n\n  protected override defaultHeaders(): Core.Headers {\n    return {\n      ...super.defaultHeaders(),\n      'OpenAI-Organization': this.organization,\n      ...this._options.defaultHeaders,\n    };\n  }\n\n  protected override authHeaders(): Core.Headers {\n    return { Authorization: `Bearer ${this.apiKey}` };\n  }\n\n  static OpenAI = this;\n\n  static APIError = Errors.APIError;\n  static APIConnectionError = Errors.APIConnectionError;\n  static APIConnectionTimeoutError = Errors.APIConnectionTimeoutError;\n  static APIUserAbortError = Errors.APIUserAbortError;\n  static NotFoundError = Errors.NotFoundError;\n  static ConflictError = Errors.ConflictError;\n  static RateLimitError = Errors.RateLimitError;\n  static BadRequestError = Errors.BadRequestError;\n  static AuthenticationError = Errors.AuthenticationError;\n  static InternalServerError = Errors.InternalServerError;\n  static PermissionDeniedError = Errors.PermissionDeniedError;\n  static UnprocessableEntityError = Errors.UnprocessableEntityError;\n}\n\nexport const {\n  APIError,\n  APIConnectionError,\n  APIConnectionTimeoutError,\n  APIUserAbortError,\n  NotFoundError,\n  ConflictError,\n  RateLimitError,\n  BadRequestError,\n  AuthenticationError,\n  InternalServerError,\n  PermissionDeniedError,\n  UnprocessableEntityError,\n} = Errors;\n\nexport import toFile = Uploads.toFile;\nexport import fileFromPath = Uploads.fileFromPath;\n\nexport namespace OpenAI {\n  // Helper functions\n  export import toFile = Uploads.toFile;\n  export import fileFromPath = Uploads.fileFromPath;\n\n  export import Page = Pagination.Page;\n  export import PageResponse = Pagination.PageResponse;\n\n  export import Completions = API.Completions;\n  export import Completion = API.Completion;\n  export import CompletionChoice = API.CompletionChoice;\n  export import CompletionUsage = API.CompletionUsage;\n  export import CompletionCreateParams = API.CompletionCreateParams;\n  export import CompletionCreateParamsNonStreaming = API.CompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsStreaming = API.CompletionCreateParamsStreaming;\n\n  export import Chat = API.Chat;\n\n  export import Edits = API.Edits;\n  export import Edit = API.Edit;\n  export import EditCreateParams = API.EditCreateParams;\n\n  export import Embeddings = API.Embeddings;\n  export import CreateEmbeddingResponse = API.CreateEmbeddingResponse;\n  export import Embedding = API.Embedding;\n  export import EmbeddingCreateParams = API.EmbeddingCreateParams;\n\n  export import Files = API.Files;\n  export import FileContent = API.FileContent;\n  export import FileDeleted = API.FileDeleted;\n  export import FileObject = API.FileObject;\n  export import FileObjectsPage = API.FileObjectsPage;\n  export import FileCreateParams = API.FileCreateParams;\n\n  export import Images = API.Images;\n  export import Image = API.Image;\n  export import ImagesResponse = API.ImagesResponse;\n  export import ImageCreateVariationParams = API.ImageCreateVariationParams;\n  export import ImageEditParams = API.ImageEditParams;\n  export import ImageGenerateParams = API.ImageGenerateParams;\n\n  export import Audio = API.Audio;\n\n  export import Moderations = API.Moderations;\n  export import Moderation = API.Moderation;\n  export import ModerationCreateResponse = API.ModerationCreateResponse;\n  export import ModerationCreateParams = API.ModerationCreateParams;\n\n  export import Models = API.Models;\n  export import Model = API.Model;\n  export import ModelDeleted = API.ModelDeleted;\n  export import ModelsPage = API.ModelsPage;\n\n  export import FineTunes = API.FineTunes;\n  export import FineTune = API.FineTune;\n  export import FineTuneEvent = API.FineTuneEvent;\n  export import FineTuneEventsListResponse = API.FineTuneEventsListResponse;\n  export import FineTunesPage = API.FineTunesPage;\n  export import FineTuneCreateParams = API.FineTuneCreateParams;\n  export import FineTuneListEventsParams = API.FineTuneListEventsParams;\n  export import FineTuneListEventsParamsNonStreaming = API.FineTuneListEventsParamsNonStreaming;\n  export import FineTuneListEventsParamsStreaming = API.FineTuneListEventsParamsStreaming;\n}\n\nexport default OpenAI;\n"],
  "mappings": ";;;;;AAAO,IAAM,UAAU;;;ACUjB,IAAO,SAAP,MAAa;cAGjB,UAAsC,YAAqC;SAAvD,WAAA;SAA4B,aAAA;SAC9C,UAAK,IAAU,WAAI;;SAGR,eAAc;QACzB,CAAA,KAAK,SAAK,MAAS;sBACZ,MAAW;YAChB,IAAA,MAAU,mDAAM;;UAElB,cAAM,IAAc,YAAI;UAExB,OAAM,4BAAO,KAAmC,SAAK,IAAS;qBACnD,SAAW,MAAI;iBACnB,QAAM,YAAQ,OAAY,KAAM,GAAC;oBACpC,KAAS,QAAQ,OAAO,IAAC;;AACzB,gBAAO;;;eAEV,QAAA,YAAA,MAAA,GAAA;YAED,MAAK,KAAU,QAAI,OAAW,IAAC;;AAC7B,cAAS;;;UAEV,OAAA,aAAA,IAAA;AACH,QAAC,OAAA;AAED,QAAA;uBACa,OAAM,KAAA,aAAA,GAAA;AACjB,YAAI;AAAA;YACF,IAAI,KAAK,WAAW,QAAQ,GAAC;iBAC3B;;;0BAGM,MAAQ;;kBAEb,KAAA,MAAA,IAAA,IAAA;mBAED,GAAI;oBACF,MAAI,sCAAA,IAAA,IAAA;0BACF,eAAiB,IAAI,GAAA;;;;;;;uBAO1B,SAAA,EAAA,SAAA;AAAA;YACD;;;AAEA,aAAA,WAAA,MAAA;;;;uBAGD;gBAAS;iBACR;gBACI,CAAC;kBAAM,CAAA;;SAEd,MAAA;AACF,QAAA,KAAA,SAAA,IAAA,GAAA;AAEK,aAAA,KAAU,UAAA,GAAA,KAAA,SAAA,CAAA;IAKd;QACE,CAAA,MAAK;UAEL,CAAA,KAAK,SAAY,CAAA,KAAA,KAAA;AAAA,eAAA;AAClB,YAAA,MAAA;QAED,OAAmB,KAAA;QACjB,MAAI,KAAK,KAAA,KAAS,IAAO;aACvB,KAAI;;WAGN,QAAW;kBACT,CAAA;oBACK,CAAI;;;gBAGP,KAAK,IAAE;wBACD,GAAK,GAAA;;;oBAIR,GAAK,KAAG,IAAK,UAAA,MAAA,GAAA;cAClB,WAAY,GAAG,GAAA;cACf,MAAK,UAAY,CAAA;;sBAGlB,SAAA;WAED,QAAW;eAEP,cAAgB,QAAM;gBACxB,KAAO,KAAK;;WAGd;;;qCAUC;gBAEM;AACT,SAAC,SAAA,CAAA;AACF,SAAA,aAAA;EAED;;;;;AAKG,WAAA,aAAA;IACH;AASE,QAAA,KAAA,SAAA,IAAA,GAAA;WACE,aAAgB;aACX,KAAA,MAAU,GAAG,EAAA;IACpB;AAEA,QAAA,CAAA,MAAO;aACD,CAAA;;4BAGY,aAAK,cAAA,IAAA,KAAA,KAAA,SAAA,CAAA,KAAA,EAAA;gBACf,KAAC,MAAU,aAAS,cAAA;cACzB,WAAA,KAAA,CAAA,iBAAA;WACD,OAAS,KAAA,MAAS,CAAI,CAAC;aACrB,CAAA;;aAED,OAAA,SAAA,GAAA;cAEI,CAAA,KAAM,OAAA,KAAA,EAAA,IAAA,MAAA,CAAA,GAAA,GAAA,MAAA,MAAA,CAAA,CAAA;oBACF,CAAE;;QAGX,CAAA,iBAAM;WACN,SAAY,CAAA,MAAK,IAAM,KAAA,EAAA;;;;aAKtB,OAAA;QAEDA;iBACO;AAAI,aAAK;eACd,UAAW;AAAM,aAAA;QAGnB,OAAK,WAAA,aAAiB;2BACL,QAAM;eACtB,MAAA,SAAA;;AAGF,UAAA,iBAAA,YAAA;AAED,eAAW,OAAY,KAAA,KAAA,EAAA,SAAA;;YACjB,IAAA;gDAAyB,MAAA,YAAA,IAAA;;;QAI7B,OAAI,gBAAkB,aAAa;2BACxB,cAAoB,iBAAA,aAAA;oBAC3B,iBAAa,QAAWA,QAAA,SAAAA,MAAA,KAAA,cAAA,IAAA,YAAA,MAAA;eACzB,KAAA,YAAA,OAAA,KAAA;;gBAEC;4DACD,MAAA,YAAA,IAAA;;;UAOH,IAAA;;;;;cAKG,OAAA,UAAA,CAAA,KAAA,YAAA;aAED,CAAA;;UAOF,QAAU,CAAA,KACR,OAAA,KAAA,EAAA,CAAA;AAEJ,SAAC,SAAA,CAAA;AAED,SAAK,aAAA;WACH;;;YAKI,gBAAa,oBAAA,IAAA,CAAA,MAAA,MAAA,MAAA,MAAA,KAAA,KAAA,KAAA,KAAA,UAAA,QAAA,CAAA;YACb,iBAAc;SAClB,UAAO,KAAM,WAAA;QACd,QAAA,IAAA,QAAA,SAAA;;AA/FD,WAAA,CAAA,IAAA,UAAkB,GAAA,KAAA,GAAA,WAAA,IAAA,UAAA,QAAA,UAAA,MAAA,CAAA;EACX;AACA,SAAA,CAAA,KAAA,IAAA,EAAA;AAgGT;AASA,SAAA,4BAAA,QAAA;;;;;;AAKG,UAAA;AACH,cAAS,SAAA,MAAA,OAA2B,KAAI;AAClC,YAAA,WAAc,QAAA,WAAc,SAAA,SAAA,OAAA;AAAA,iBAAA,YAAA;AAAE,eAAO;MAEzC,SAAM,GAAM;AACZ,eAAO,YAAA;AACL,cAAM;;;mBAGF;4BAAyB,OAAA,OAAe;yBACjC;;aACP,EAAA,MAAQ,MAAE,OAAA,OAAA;;yBAEF,IAAA;aACT;;;;;;ACnQP;;;;;;;;;;;;;;;AAIM,IAAO,WAAP,MAAO,kBAAiB,MAAK;cASjC,QAC4B,OACD,SACE,SACC;UAE5B,UAAM,YAAS,OAAY,OAAO,CAAA;SAClC,SAAK;SACL,UAAK;UAEL,OAAM;SACN,QAAK;SACL,OAAK,SAAW,QAAA,SAAA,SAAA,SAAA,KAAJ,MAAO;SACnB,QAAK,SAAY,QAAA,SAAA,SAAA,SAAA,KAAJ,OAAO;SACpB,OAAK,SAAW,QAAA,SAAA,SAAA,SAAA,KAAJ,MAAO;;SAGb,YAAO,OAAsB,SAA6B;sBAI1D,QAAK,UAAU,SAAM,SAAQ,MAAA,wBAE/B,YAAW,WAAA,MAAA,UAEhB,KAAA,UAAA,MAAA,OAAA,IAED,QAAO,KACL,UACA,KAAA,IAIA,WAAW;;SAIX,SAAM,QAAS,eAAqC,SAArC,SAAA;QAEf,CAAA,QAAI;aACF,IAAA,mBAAW,EAAe,OAAO,YAAS,aAAgB,EAAE,CAAA;;UAG9D,QAAU,kBAAU,QAAA,kBAAA,SAAA,SAAA,cAAA,OAAA;mBACX,KAAI;aACZ,IAAA,gBAAA,QAAA,OAAA,SAAA,OAAA;;mBAGQ,KAAI;aACZ,IAAA,oBAAA,QAAA,OAAA,SAAA,OAAA;;mBAGQ,KAAI;aACZ,IAAA,sBAAA,QAAA,OAAA,SAAA,OAAA;;mBAGQ,KAAI;aACZ,IAAA,cAAA,QAAA,OAAA,SAAA,OAAA;;mBAGQ,KAAI;aACZ,IAAA,cAAA,QAAA,OAAA,SAAA,OAAA;;mBAGQ,KAAI;aACZ,IAAA,yBAAA,QAAA,OAAA,SAAA,OAAA;;mBAGQ,KAAI;aACZ,IAAA,eAAA,QAAA,OAAA,SAAA,OAAA;;AAGH,QAAC,UAAA,KAAA;AACF,aAAA,IAAA,oBAAA,QAAA,OAAA,SAAA,OAAA;IAED;AAGE,WAAA,IAAA,UAAc,QAAoC,OAAA,SAAA,OAAA;;;AAEjD,IAAA,oBAAA,cAAA,SAAA;EACF,YAAA,EAAA,QAAA,IAAA,CAAA,GAAA;AAED,UAAM,QAAO,QAAmB,WAAQ,wBAAQ,MAAA;AAG9C,SAAA,SAAc;;;IAEZ,mCAAA,SAAA;cACA,EAAA,SAAa,MAAA,GAAA;UACb,QAAS,QAAA,WAAA,qBAAA,MAAA;kBAAO;AAIpB,QAAM;AAAO,WAAA,QAAA;;;AAGV,IAAA,4BAAA,cAAA,mBAAA;EACF,cAAA;AAED,UAAM,EAAA,SAAO,qBAAwB,CAAA;;;IACjB,gCAAkB,SAAA;gBACrC;AAAA,UAAA,GAAA,SAAA;AAED,SAAM,SAAO;;;IACO,oCAAkB,SAAA;gBACrC;AAAA,UAAA,GAAA,SAAA;AAED,SAAM,SAAO;;;IACO,sCAAkB,SAAA;gBACrC;AAAA,UAAA,GAAA,SAAA;AAED,SAAM,SAAO;;;IACO,8BAAkB,SAAA;gBACrC;AAAA,UAAA,GAAA,SAAA;AAED,SAAM,SAAO;;;IACO,8BAAkB,SAAA;gBACrC;AAAA,UAAA,GAAA,SAAA;AAED,SAAM,SAAO;;;IACO,yCAAkB,SAAA;gBACrC;AAAA,UAAA,GAAA,SAAA;AAED,SAAM,SAAO;;;IACO,+BAAkB,SAAA;gBACrC;AAAA,UAAA,GAAA,SAAA;AAED,SAAM,SAAO;EAAuC;;;;;;ACpI7C,IAAM,kBAAkB,CAAC,QAAoB;SAClD;AACF;;;ACJA,IAAM,SAAS,MAAM,KAAK,MAAS;AAK5B,IAAM,eAAe;;;ACR5B,IAAM,YAAY;AAGlB,IAAM,QACJ,OAAO,SAAS,cAAc;AAAA;AAAA,EAE5B,MAAMC,cAAa,KAAK;AAAA,IACtB,cAAc;AACZ,YAAM,IAAI,MAAM,gFAAgF;AAAA,IAClG;AAAA,EACF;AAAA;;;ACNJ,eAAsB,2BACpB,MACA,MAAuB;SAEvB,EAAO,GAAE,MAAO,MAAM,IAAE,cAAI,IAAc,EAAI;AAChD;;;ACWA,eAAsB,eAAY;QAChC,IAAM;IAGP;;;;;ACDK,SAAU,eAAe,OAAU;SACvC;AACF;;;AC4BO,IAAM,iBAAiB,CAAC,mBAEtB,eACP,UAAa,mBACb,MAAO,QAAU,YAEnB,OAAO,MAAM,SAAU;AAErB,IAAO,aAAU,CAAA,mBACV,eACP,UAAa,mBACb,MAAW,SAAO,YAEpB,OAAA,MAAA,iBAAA;AAOE,IAAO,aAAe,CAAA,mBACf,eACP,UAAa,mBACb,MAAO,SAAW,mBAClB,MAAO,SAAM,YAEf,OAAO,MAAM,SAAA,qBACX,MAAO,UAAW,cAClB,OAAA,MAAA,gBAAA;AAIF,IAAA,eAAA,CAAA,UAAA;;;eAegB,OAAM,OAAA,MAAA,UAAA,CAAA,GAAA;MAEpBC,KAAI,IAAA;UAEF,MAAI;qBAEO,KAAM,GAAW;UAC7B,OAAA,MAAA,MAAA,KAAA;AAED,aAEA,QAEKA,MAAA,IAAQ,IAAI,MAAE,GAAA,EAAA,SAAA,MAAA,OAAA,EAAA,IAAA,OAAA,QAAAA,QAAA,SACjBA,MACA;eACE,MAAO,CAAA,IAAK,GAAG,MAAO,OAAM;;QAE/B,OAAA,MAAA,SAAA,KAAA;WAEM,QAAQ,KAAK,QAAQ,KAAA,OAAS,QAAA,OAAA,SAAA,KAAA;AACtC,MAAA,CAAA,QAAA,MAAA;AAED,UAAK,QAAU,KAAQ,KAAC,CAAkB,OAAA,QAAA,OAAA,SAAA,SAAA,GAAA;;AACxC,gBAA6B,EAAE,GAAC,SAAA,KAAA;IAChC;;SAGE,IAAK,MAAA,MAAA,MAAY,OAAW;;eAG7B,SAAA,OAAA;;cACO,CAAA;aACD,UACL,wBACA,OAAA,KAAA;mBACW;UAEV,KAAA,KAAA;aACF,WAAA,KAAA,GAAA;UAAM,KAAA,MAAA,MAAA,YAAA,CAAA;qCAMN,KAAA,GAED;AACD,qBAAA,SAAA,OAAA;AAED,YAAS,KAAA,KAAa;IACpB;SACA;AACD,UAAA,IAAA;MAED,yBAA2B,OAAA,KAAA,uHACzB,SAEEA,IAAA;;;AAIH,SAAA;AAED;SACM,cAAa,OAAQ;QAAE,QAAQ,OAAC,oBAAA,KAAA;SAChC,IAAA,MAAO,IAAM,CAAA,MAAK,IAAA,CAAA,GAAW,EAAA,KAAK,IAAA,CAAA;;SACtC,QAAO,OAAU;AACjB,MAAAA;AAEF,SAGA,yBAA0B,MAAA,IAAA,KACxB,yBAA4C,MAAA,QAAA;IAAIA,MAAA,yBAAA,MAAA,IAAA,OAAA,QAAAA,QAAA,SAAA,SAAAA,IAAA,MAAA,OAAA,EAAA,IAAA;;IAG/C,2BAAA,CAAA,MAAA;AACF,MAAA,OAAA,MAAA;AAAA,WAAA;AAED,MAAM,OAAO,WAAA,eAAqD,aACxD;AAAW,WAAK,OAAQ,CAAA;AAElC,SAAA;;kDAGG,QAAA,OAAA,UAAA,YAAA,OAAA,MAAA,OAAA,aAAA,MAAA;AACI,IAAM,gBAAN,MAAM;cAGN,MAAA;SAA+B,OAAO;;OAG3C,OAAO,WAAA,IAAA;AACP,WAAA;EAEF;;AAIE,IAAO,kBAAA,CAAA,SACP,QAAA,OAAA,SAAA,YAAA,KAAA,QAAA,KAAA,OAAA,WAAA,MAAA;IAU0B,8BAAkB,OAAoB,SAAA;QAC5D,OAAK,MAAI,WAAY,KAAK,IAAA;SAC5B,2BAAuB,MAAA,IAAA;;iBAC6B,OAAK,SAAA;eACxD,IAAA,UAAA;QACF,QAAA,IAAA,OAAA,QAAA,QAAA,CAAA,CAAA,EAAA,IAAA,CAAA,CAAA,KAAA,KAAA,MAAA,aAAA,MAAA,KAAA,KAAA,CAAA,CAAA;SACD;AACF;mBAaa,OAAa,MAAQ,KAAA,UAAA;gBACxB;AAAa;eACd,MAAO;UACb,IAAA;4BAAwB,GAAA;;;aAGvB,UAAc,YACL,OAAQ,UAAW,YAAY,OAAM,UAAA,WAAsB;SAErE,OAAA,KAAA,OAAA,KAAA,CAAA;aAAM,aAAA,KAAA,GAAA;UACL,OAAM,MAAI,OACR,KAAA;SAEH,OAAA,KAAA,IAAA;EACD,WAAA,MAAA,QAAA,KAAA,GAAA;;;;;;;;;;;;;;;;;;;;;;;sCCnPA,UACA,OAAY,MAAI,GAAA;MAMX,SAAE,OAAe,CAAA;AAAE,UAAA,IAAA,UAAA,+CAAA;MACnB,OACL,UAAA,aAAA,aACA,SAAA,CAAA,IAAA,CAAA,MACA,IAAA,QAAU;AAIN,UAAA,IAAA,UAAgB,0EAAA;AAYtB,SACE,SAAQ,MAAQ,IACZ,SAAM,MAAQ,EAAA,KAAQ,QAAA,IACxB,IAAA,EAAA,QACA,MAAA,IAAA,QAAA;;IAIF;IAWA,cAAgB;eACA,qBAAC,OAAA;AAClB,QAAA,EAAA,SAAA,IAAA;AAED,MAAA,MAAA,QAAA,QAAA;AAGG,WAAA,IAAA,OAAA,UAAA,MAAA,UAAA;EACH;QAGE,cACU,SAA0C,QAC1C,IAAA,cAAgE;sBAE1D,QAAI,gBAAA,SAAA,SAAA,YAAA,SAAA,kBAAA,GAAA;iBAChB,MAAA,SAAA,KAAA;sBACA,SAAA,QAAA,SAAA,KAAA,SAAA,SAAA,IAAA;;;QALM,OAAA,MAAA,SAAe,KAAf;QACA,YAAA,SAAA,QAAA,SAAoF,KAAA,SAAA,SAAA,IAAA;SAQ7F;;;;;cAcC,IAAO;IACT,CAAC;AACD,SAAA,kBAAA;;;;;;;;;;;;;eAcG;WACD,KAAO,gBAAK,KAAc,CAAA,MAAA,EAAA,QAAA;;;;;;;;QAgBnB,eAAmD;UAC1D,CAAA,MAAO,QAAU,IAAG,MAAO,QAAC,IAAW,CAAA,KAAA,MAAA,GAAA,KAAA,WAAA,CAAA,CAAA;AACzC,WAAC,EAAA,MAAA,SAAA;EACF;EAED,QAAM;AASJ,QAAA,CAAA,KAAA,eAEE;AAEA,WAAA,gBACO,KAAA,gBAOR,KAAA,KAAA,aAAA;;WAEC,KAAK;;oBAEA,YAAY;WAEjB,KAAK,MAAQ,EAAA,KAAA,aAAc,UAAd;;QAGL,YAAW;WACnB,KAAO,MAAG,EAAA,MAAA,UAAA;;UAGZ,WAAA;;;;;;;;IAQU,UAAA;;;;;mBAIN;sBACG;;qBAEH,QAAA,eAAA,SAAA,aAAA;IACJ;AAIA,SAAA,UAAA,wBAAA,WAAA,OAAA;;SAEG,QAAA,mBAAA,QAAA,mBAAA,SAAA,iBAAA;;gBAGO;WACR,CAAA;;;;;;;;;;mBAaD;AAED,WAAyB;cACvB;MACD,gBAAA;MAED,cAA0C,KAA0C,aAAA;SAClF,mBAAY;MACb,GAAA,KAAA,YAAA;IAEO;;;;;kBAcP,SAAA,eAAA;EAAA;0BAE6B;WAC5B,wBAAoB,MAAU,CAAA;;kBAE1B;gBACD,cAAA,OAAA,MAAA,IAAA;;mBAGC;gBACA,cAAa,QAAG,MAAQ,IAAO;;cAEhC,MAAA;WACF,KAAA,cAAA,SAAA,MAAA,IAAA;;MAGF,MAAA,MAAA;AAED,WAAA,KAAY,cACuB,OAAA,MAAA,IAAA;;SAEjC,MAAM,MAAE;WAER,KAAM,cACJ,UAAgB,MAAQ,IAAK;;gBAE7B,QAAO,MAAA,MAAA;WACT,KAAM,QAAA,QAAgB,QAAK,IAAA,EAAA,KAAA,CAAAC,WAA2B,EAAE,QAAA,MAAA,GAAAA,MAAA,EAAA,CAAA;;aAGpD,MAAAC,OAAa,MAAA;gBAAS,eAAAA,OAAwB,EAAA,QAAW,OAAO,MAAC,GAAQ,KAAC,CAAA;;yBAE/D,MAAG;QAClB,OAAM,SAAA,UAAkB;UACxB,OACE,WAAQ,aAAiB;eACzB,OAAA,WAAmB,MAAC,MAAA,EAAkB,SAAQ;;iBAG9C,gBAAA,aAAA;cACA,UAAA,IAAA,YAAA;cACA,UAAA,QAAA,OAAA,IAAA;eACC,QAAkB,OAAQ,SAAO;;;;;eAKlC,SAAY;aACb,IAAA,IAAA,IAAA,IAAA;UAED,EAAA,QAAM,MAAU,OAA2B,UAAA,CAAA,EAAA,IAAA;iCAEjC,QAAA,IAAc,IAAE,QAAA,KAAA,eACrB,OAAO,KAAA,UAAA,QAAA,MAAA,MAAA,CAAA;UAEZ,gBAAA,KAAA,uBAAA,IAAA;UACA,MAAI,KAAA,SAAgB,MAAQ,KAAK;qBACxB;AAAW,8BAAgB,WAAA,QAAA,OAAA;UACnC,WAAAC,MAAA,QAAA,aAAA,QAAAA,QAAA,SAAAA,MAAA,KAAA;UAED,aAGA,MAAM,KAAmB,QAAA,eAAA,QAAA,OAAA,SAAA,KAAA,KAAA,eAAA,eACvB,8BAGI,GAAS;4BACb,UAAA;iBAGD,KAAC,cAAA,QAAA,cAAA,SAAA,SAAA,UAAA,aAAA,QAEF,OAAK,SAGN,SAED,GAAA,aAAA;AAQU,gBAAA,QACR,UACyB;;AAK3B,QAAC,KAAA,qBAAA,WAAA,OAAA;AAED,UAAA,CAAO,QAC4C;AACjD,gBAAkC,iBAAI,KAAA,sBAAA;cAEtC,KAAW,iBAAgB,IAAA,QAAY;IACzC;AAEQ,UAAM,aACZ;;SAGA,KAAM,eAAU;SAChB;;QAIA,gBAAgB,QAAE,IAAS,KAAG,CAAI,cAAc;aAEhD,WAAW,cAAkB;;gBAK3B,UAAU,EAAA,QAAA,CAAA,QAAoB,WAAA,GAAA,MAAA,QAAA,OAAA,WAAA,GAAA,CAAA;UAC/B,MAAA;;SAGD,QAAM,EAAA,KAAiB;eAEnB;sBACE,EAAA,OAAQ,UAAM;;;eAGlB,KAAI,QAAA,YAAkB,QAAA,OAAA,SAAA,KAAA;;yBAErB,YAAA,OAAA;aACD,KAAI,KAAA,QAAa;;;;;;;;uBAUR,SAAK,EAAA,IAAA,GAAa;EAAA;kBAC1B,QAAA,OAAA,SAAA,SAAA;oBAEK,SAAU,QAAM,OAAS,SAAO,OAAS;;mBAEzC,mBAAoB,MAAG;eAE7B,WAAM,KAAY,YAAS,SAAa,gBAAe,CAAE;;oBAGnD,cAAI,kBAAA;aACX,IAAA;UAED,UAAS,MAAQ;AACnB,QAAC,oBAAA,MAAA;AAED,0BAEEA,MAAA,QAA4B,gBAAA,QAAAA,QAAA,SAAAA,MAAA,KAAA;;UAG5B,EAAA,KAAO,KAAI,QAAW,IAAkB,KAAM,aAAe,OAAA;AAC/D,UAAC,KAAA,eAAA,KAAA,EAAA,IAAA,CAAA;AAED,UAAA,WAA4B,KAAsB,SAAA,IAAA,OAAA;SAChD,KAAM,QACJ,YAAc,QAAO,OAAA,SAAA,SAAA,GAAA,SAAA;YACnB,IAAI,kBAAS;;UAGjB,aAAM,IAAY,gBAAQ;UAC1B,WAAK,MAAW,KAAA,iBAAe,KAAA,KAAA,SAAA,UAAA,EAAA,MAAA,WAAA;4BAChB,OAAA;WACd,KAAA,QAAA,YAAA,QAAA,OAAA,SAAA,SAAA,GAAA,SAAA;AAED,cAAI,IAAK,kBAAE;;UAEV,kBAAA;AAED,eAAO,KAAI,aAAW,SAAA,gBAAA;MACvB;AAES,UAAA,SAAe,SAA8B,cAAA;AACrD,cAAA,IAAO,0BAAqB;;gBAEtB,mBAAkB,EAAA,OAAA,SAAA,CAAA;;4BAER,sBAAuB,SAAI,OAAA;kBACtC,IAAA;8BACiB,KAAE,YAAA,QAAA,GAAA;oBAClB,aAAU,SAAA,kBAA2B,eAAA;;YAEvC,UAAU,MACR,SAAA,KAAA,EAAA,MAAA,MAAyB,SAAY;YAEvC,UAAA,SAAA,OAAA;yBACS,UAAA,SAAA;AACd,YAAA,YAAA,SAAA,QAAA,KAAA,iBAAA,UAAA;AAED,YAAM,MAAA,KAAA,gBAGJ,SACA,QAA2B,SAAA,YAAA,eAAA;YAE3B;;aACY,UAAO,SAAA,WAAwB;;iBAIpCD,OAAK,SAAA;oBACH,KAAK,YAAU,SAAW,IAAA;eAChC,YAAa,MAAA,SAAAA,KAAA;;WAEX,MAAA,OAAA;AACP,UAAC,MAES,cAAgB,IAAA,IACxB,IAAA,IAAO,IAAE,IACV,IAAA,IAAA,KAAA,WAAA,KAAA,QAAA,SAAA,GAAA,KAAA,KAAA,WAAA,GAAA,IAAA,KAAA,MAAA,CAAA,IAAA,KAAA;AAEO,UAAA,eAA8B,KAAA,aAAA;QACpC,CAAA,WAAA,YAAA,GAAA;cACM,EAAA,GAAA,cAAiB,GAAG,MAAQ;;QAGlC,OAAI;mBAAqC,KAAK,eAAA,KAAA;;eACX,SAAY;;iBAG3C,OAAS;kBAAgB,QAAY,KAAA,SAEzC,CAAA,CAAA,GAAA,KAAA,MAAA,OAAwB,UAAA,WAAA,MACxB,CAAI,CAAA,KAAA,KAAS,MAAM;UAAU,OAAO,UAAK,YAAA,OAAA,UAAA,YAAA,OAAA,UAAA,WAAA;AAEzC,eAAA,GAAA,mBAAyB,GAAA,CAAA,IAAA,mBAAA,KAAA,CAAA;MACzB;UAA4B,UAAO,MAAK;AAExC,eAAO,GAAK,mBAAC,GAAA,CAAA;MACd;AAEO,YAAM,IAAA;;MAKZ;YAGE,GAAA;;QAEF,iBAAA,KAAA,MAAA,IAAA,YAAA;UACA,EAAA,QAAM,GAAA,QAAa,IAAQ,QAAC,CAAA;QAE5B;AAAM,aAAA,iBAAoB,SAAC,MAAU,WAAA,MAAA,CAAA;UACrC,UAAM,WAAe,MAAA,WAAA,MAAA,GAA4B,EAAC;WAClD,KAAM,iBAAe,QAErB,KAAO,EAAK,QAAA,WAAmB,QAAE,GAAA,QAAiB,CAAC,EACpD,QAAA,MAAA;AAEO,mBAAA,OAAA;;;qBAQN;WACA,EAAA,OAAA,KAAA,MAAA;;cAEE,UAAO;UAGT,oBAAmB,SAAU,QAAG,IAAA,gBAAiB;QAGjD,sBAAqB;AAAS,aAAA;QAE9B,sBAAA;AAAA,aAAA;QAGA,SAAO,WAAY;AAAG,aAAO;AAGvB,QAAA,SAAY,WAAA;AAAA,aAAA;AAEpB,QAAC,SAAA,UAAA;AAAA,aAAA;AACF,WAAA;EAED;QAEE,aAAY,SAAiB,kBAAA,iBAAA;QAC3BC;wBAEW;UAKX,aAAe;OAChB,oBAAA,QAAA,oBAAA,SAAA,SAAA,gBAAA,aAAA,MAQF;IAID;AAOE,UAAA,cAA+BA,MAAA,QAAoB,gBAA2C,QAAAA,QAAA,SAAAA,MAAA,KAAA;UAN9F,UAAA,KAAA,6BAAmB,kBAAA,YAAA,UAAA,IAAA;UAOjB,MAAA,OAAA;WACA,KAAK,YAAU,SAAQ,gBAAA;;+BAEN,kBAAA,YAAA,YAAA;AACnB,UAAC,oBAAA;AAUD,UAAA,gBAAW;eAEU,UAAa,UAAA,KAAA,cAAA,IAAA;aAChC;IACF;AAEA,UAAM,aAAW,aAAA;UAEf,eAAe,KAAA,IAAA,oBAAA,KAAA,IAAA,aAAA,GAAA,CAAA,GAAA,aAAA;UAId,SAAA,KAAA,OAAA,IAAA;WACD,eAAiB;;iBAEf;WACD,GAAA,KAAA,YAAA,IAAA,OAAA,OAAA;;;IAcD,qBAAW;cACX,QAAY,UAAW,MAAI,SAAA;yBACZ,IAAI,MAAC,MAAW;2BAClB,MAAA,sBAAA,QAAA,GAAA;SACZ,UAAA;AACH,SAAC,WAAA;AAED,SAAK,OAAE;;gBAEH;kBACE,KAAM,kBAAK;eACZ;AAAA,aAAA;WACF,KAAA,aAAA,KAAA;;EAEJ,MAAA,cAAA;AAED,UAAA,WAAA,KAAA,aAAA;;;;;;;;AAQG,kBAAA,QAAA,EAAA,GAAA,YAAA,OAAA,GAAA,SAAA,OAAA;IACH,WAAa,SAIX,UAAQ;AAGR,YAAA,SACE,CAAiB,GACjB,OAAkC,QAC0C,YAAA,SAAA,CAAA,CAAA,GAAA,GAAA,SAAA,IAAA,aAAA,QAAA,CAAA;iBAG1E,CAAA,KACA,KAAK,KAAE,QAAU;AAEpB,iBAAA,IAAA,aAAA,IAAA,KAAA,KAAA;MAED;;;;;;;IAOA;;SAEE,YAAW;eAEV;AACH,UAAC;AACF,WAAA,KAAA,YAAA,GAAA;AAEK,aAAO,MAAA,KAAA,YAAqB;AAGhC,YAAO;IAEH;;WAII,uBAAY,oBAAA,QAAA,GAAA,OAAA,cAAA,IAAA;qBACL,QAAQ,KAAA,UAAW,GAAA;iBAC5B,QAAa,KAAK,kBAAkB,GAAA;AACtC,cAAC;MAEJ;IACD;EAwBF;AACA;AAWE,IAAS,cAAT,cAAa,WAAA;cACJ,QAAM,SAAAC,OAAA;AACf;MACA;MACA,OAAA,UAAA,IAAAA,MAAA,QAAA,MAAA,UAAA,MAAA,qBAAA,KAAA,GAAA,MAAA,OAAA;IAEF;;;;;;;;;iBAuCM,aAAkB,IAAE;iBACpB,MAAA;qBACA,QAAgB,MAAE;YAClB;;;;AAIH,IAAA,wBAAA,CAAA,YAAA;SACG,IAAA;WACF;;cAEE,QAAA;;;kBAGA,MAAA;cACA,MAAA,KAAA,SAAA;eACA,OAAA,IAAA,YAAA,CAAA,KAAA,OAAA,GAAA;MACH;IACD;;;4BA6BsB,MAAA;aACpB,SAAA,eAAuB,KAAS,SAAA,MAAA;WAChC;MACA,oBAAA;MACF,+BAAA;MASF,kBAAA,kBAAA,KAAA,MAAA,EAAA;MACA,oBAAuB,cAAA,KAAA,MAAA,IAAA;MACrB,uBAAyB;qCACX,KAAA;;;MAId,OAAM,gBAAkB,aAAA;WACpB;0BACoB;qCACS;wBACP;0BACG,SAAS,WAAA;6BACV;MAC1B,+BAAA,QAAA;IAEF;;aAGM,UAAO,SAAA,KAAA,OAAA,YAAA,cAAA,UAAA,CAAA,MAAA,oBAAA;;0BAEK;qCACQ;wBAEb,kBAAqB,QAAK,QAAS;0BAC7C,cAAA,QAAA,IAAA;MACF,uBAAA;MAED,+BAAY,QAAA;IACb;EAED;QACE,cAAa,eAAA;MACb,aAAA;AACA,WAAA;MACA,oBAAA;MACA,+BAAkB;wBAAe;MACjC,oBAAqB;6BAAiC,WAAA,YAAA,OAAA;MACtD,+BAAkB,YAAA;;;SAEd;wBAAsB;IAC1B,+BAAiB;IACjB,kBAAA;IAEF,oBAAuB;IACrB,uBAAkB;IAClB,+BAAA;;;SAKA,iBAAoB;MAEpB,CAAA,aAAA,OAAA,cAAA,aAAA;AACA,WAAA;;QAGI,kBAAkB;WAAQ,QAAO,SAAM,uCAAA;IAC3C,EAAA,KAAI,MAAQ,SAAK,uCAAS;WAAE,MAAO,SAAU,6CAAA;IAC7C,EAAA,KAAI,UAAa,SAAQ,yCAAA;WAAE,WAAc,SAAC,0CAAA;IAC1C,EAAA,KAAI,UAAa,SAAO,oEAAA;;aACI,EAAO,KAAA,QAAU,KAAA,iBAAA;AAC7C,UAAI,QAAQ,QAAK,KAAS,UAAA,SAAA;QAAE,OAAO;AACnC,YAAI,QAAQ,MAAK,CAAA,KAAO;YAAE,QAAO,MAAQ,CAAA,KAAA;AACzC,YAAI,QAAQ,MAAA,CAAA,KAAA;aAAE,EAAO,SAAS,KAAA,SAAW,GAAA,KAAA,IAAA,KAAA,IAAA,KAAA,GAAA;IACzC;EACA;AAEF,SAAI;AACJ;IACE,gBAAQ,CAAA,SAAgB;MAMvB,SAAA;AAAA,WAAA;MAAC,SAAU,YAAE,SAAA;AAAA,WAAA;eACL;AAAA,WAAU;MAClB,SAAA,aAAA,SAAA;AAAA,WAAA;AACD,MAAA;AAAA,WAAA,SAAA,IAAA;AAEF,SAAA;AACA;AACA,IAAM,oBAA4B,CAAA,aAAa;aAQ3C,SAAU,YAAa;MAKzB,SAAS,SAAA,KAAA;AAAA,WAAA;AACT,MAAA,aAAA;AAAA,WAAA;AAEF,MAAM,aAAO;AAAe,WAAmB;MAC7C,aAAO;AAAiB,WAAA;mBAAa;AAAA,WAAA;MACrC,aAAW;AAAW,WAAA;AACtB,MAAA,aAAA;AAAA,WAAA;AAEF,MAAM;AAAO,WAAA,SAAoB,QAAkC;SAC7D;;IACJ;AACF,IAAE,qBAAA,MAAA;AAEF,SAAA,qBAAA,QAAA,qBAAA;;AAIG,IAAA,WAAA,CAAA,SAAA;AACH,MAAM;;WACA,KAAO;WACT;;;IAID,yBAAA,IAAA,OAAA,mBAAA,GAAA;IACD,gBAAiB,CAAA,QAAA;AACjB,SAAA,uBAAA,KAAA,GAAA;AAEF;IACE,QAAI,CAAO,OAAK,IAAK,QAAQ,CAAA,YAAA,WAAA,SAAA,EAAA,CAAA;8BAA0B,CAAC,MAAA,MAAA;MACxD,OAAI,MAAO,YAAU,CAAA,OAAQ,UAAA,CAAA,GAAA;UAAE,IAAA,MAAO,GAAQ,IAAC,qBAAW;;AAG1D,MAAA,IAAA,GAAA;AAEF,UAAO,IAAM,MAAA,GAAA,IAAc,6BAA2B;;SACrB;;IACA,cAAkB,CAAA,QAAO;MAExD,eAAgB;AAAA,WAAA;AAChB,SAAA,IAAA,MAAA,GAAA;AAEF;AASG,IAAA,UAAA,CAAA,QAAA;MACDC,KAAA,IAAO,IAAA;AACP,MAAA,OAAA,YAAA,aAAA;AAEF,YAAO,MAAMA,MAAA,QAAgB,SAAwC,QAAEA,QAAA,SAAA,SAAAA,IAAA,GAAA,OAAA,QAAA,OAAA,SACjE,KACF;;MAEF,OAAO,SAAW,aAAQ;AAC1B,YAAA,MAAA,KAAA,KAAA,SAAA,QAAA,OAAA,SAAA,SAAA,GAAA,SAAA,QAAA,OAAA,SAEK,SACD,GAAA,KAAK,IAAK,GAAA;;SAEb;;SAuCC,WAAa,KAAA;;AACb,WAAO;AAET,aAAA,MAAA;AAAA,WAAA;AAOF,SAAO;;SAMkC,MAAO,WAAS,MAAI;MAC3D,OAAM,YAAe,eAAS,QAAQ,IAAU,OAAA,MAAA,QAAA;AAChD,YAAI,IAAM,gBAAgB,MAAA,IAAA,GAAA,IAAA;;;IAK1B,QAAO,MAAM;AACb,SAAA,uCAAA,QAAA,SAAA,CAAA,MAAA;AAEF,UAAA,IAAA,KAAA,OAAA,IAAA,KAAA;;AAEG,WAAA,EAAA,SAAA,EAAA;EACH,CAAA;;IACY,qBAAU,MAAA;;;WAGnB,WAAA;WAGC,OAAO,aAAU;IAGnB,OAAM,cAAU;;;;;ACrgCZ,IAAO,OAAP,cAA0B,aAAkB;cAKhD,QAA6B,UAAoB,MAA0B,SAA8B;UACvG,QAAM,UAAQ,MAAU,OAAM;SAE9B,SAAK,KAAS;SACd,OAAK,KAAO;;sBAGG;WACf,KAAO;;;;;;;mBAQK;WACZ;;iBAGU;WACV;;;;;ACnCE,IAAO,cAAP,MAAkB;cAEtB,QAA0B;SACxB,SAAK;SAEL,MAAK,OAAM,IAAO,KAAI,MAAK;SAC3B,OAAK,OAAO,KAAO,KAAK,MAAK;SAC7B,QAAK,OAAQ,MAAO,KAAM,MAAK;SAC/B,MAAK,OAAM,IAAO,KAAI,MAAK;SAC3B,SAAK,OAAS,OAAO,KAAO,MAAK;SACjC,aAAK,OAAa,WAAO,KAAW,MAAK;;;;;ACPvC,IAAO,iBAAP,cAA8B,YAAW;;;;SAI7C,MAAsC,SAA+B;WACnE,KAAO,KAAK,yBAAK,4BAAyB,EAAA,MAA8B,GAAI,QAAK,CAAA,CAAA;;;AAkDrF,0BAAiBC,iBAAc;AAAA,GAAA,mBAAA,iBAAA,CAAA,EAAA;;;ACvDzB,IAAO,eAAP,cAA4B,YAAW;;;;SAI3C,MAAoC,SAA+B;WACjE,KAAO,KAAK,uBAAK,4BAAuB,EAAA,MAA8B,GAAI,QAAK,CAAA,CAAA;;;AA2CnF,0BAAiBC,eAAY;AAAA,GAAA,iBAAA,eAAA,CAAA,EAAA;;;AChDvB,IAAO,QAAP,cAAqB,YAAW;gBAAtC;;SACE,iBAAA,IAAiC,eAAI,KAAe,MAAK;SACzD,eAAA,IAA6B,aAAI,KAAa,MAAK;;;CAGrD,SAAiBC,QAAK;SACN,iBAAiB;SAIjB,eAAe;AAG/B,GARiB,UAAA,QAAK,CAAA,EAAA;;;ACHhB,IAAO,cAAP,cAA2B,YAAW;SAa1C,MAC8B,SACC;;WAE7B,KAAO,KAAK,qBAAK;MAGlB;MACF,GAAA;MAqbD,SAAiBC,MAAA,KAAW,YAAA,QAAAA,QAAA,SAAAA,MAAA;IAAX,CAAA;;;;;;;AC7cX,IAAO,OAAP,cAAoB,YAAW;gBAArC;;SACE,cAAA,IAA2B,YAAI,KAAY,MAAK;;;CAGlD,SAAiBC,OAAI;QACL,cAAc;AAQ9B,GATiB,SAAA,OAAI,CAAA,EAAA;;;ACFf,IAAOC,eAAP,cAA2B,YAAW;SAa1C,MAC8B,SACC;;WAE7B,KAAO,KAAK,gBAAK;MAGlB;MACF,GAAA;MA6QD,SAAiBC,MAAA,KAAW,YAAA,QAAAA,QAAA,SAAAA,MAAA;IAAX,CAAA;;;;;;;ACpSX,IAAO,aAAP,cAA0B,YAAW;;;;SAIzC,MAC6B,SACE;WAE7B,KAAO,KAAK,eAAK,EAAa,MAAI,GAAI,QAAK,CAAA;;;AA0F/C,0BAAiBC,aAAU;AAAA,GAAA,eAAA,aAAA,CAAA,EAAA;;;ACjGrB,IAAO,QAAP,cAAqB,YAAW;;;;;;;;SAQpC,MAA6B,SAA+B;WAC1D,KAAO,KAAK,UAAK,EAAQ,MAAI,GAAI,QAAK,CAAA;;;AAwF1C,0BAAiBC,QAAK;AAAA,GAAA,UAAA,QAAA,CAAA,EAAA;;;AChGhB,IAAO,QAAP,cAAqB,YAAW;;;;;;;SAOpC,MAA6B,SAA+B;WAC1D,KAAO,KAAK,UAAK,4BAAU,EAAA,MAA8B,GAAI,QAAK,CAAA,CAAA;;;;;WAMpE,QAAuB,SAA+B;WACpD,KAAO,IAAK,UAAI,MAAU,IAAM,OAAI;;;;;OAMtC,SAAkC;WAChC,KAAO,WAAK,UAAW,iBAAU,OAAiB;;;;;MAMpD,QAAkB,SAA+B;WAC/C,KAAO,OAAK,UAAO,MAAU,IAAM,OAAI;;;;;kBAMzC,QAA8B,SAA+B;WAC3D,KAAO,IAAK,UAAI,MAAU,YAAM;;eAE9B;gBACC;QACJ,GAAA,YAAA,QAAA,YAAA,SAAA,SAAA,QAAA;MACF;IAED,CAAA;;;AAoFiB,IAAA,kBAAA,cAMhB,KAAA;AAAA;;;;;ACnIK,IAAO,YAAP,cAAyB,YAAW;;;;;;;;;SASxC,MAAiC,SAA+B;WAC9D,KAAO,KAAK,eAAK,EAAa,MAAI,GAAI,QAAK,CAAA;;;;;;;WAQ7C,YAA2B,SAA+B;WACxD,KAAO,IAAK,eAAI,UAAe,IAAU,OAAI;;;;;OAM/C,SAAkC;WAChC,KAAO,WAAK,eAAW,eAAe,OAAe;;;;;SAMvD,YAAyB,SAA+B;WACtD,KAAO,KAAK,eAAK,UAAe,WAAU,OAAW;;aAqBvD,YACoB,OAC0B,SACf;;WAE7B,KAAO,IAAK,eAAI,UAAe,WAAU;;eAEvC;;eAGDC,MAAgF,UAAA,QAAA,UAAA,SAAA,SAAA,MAAA,YAAA,QAAAA,QAAA,SAClFA,MACF;IAED,CAAA;;;AAyTiB,IAAS,gBAAT,cAShB,KAAA;AAAA;;;;;ACzYK,IAAO,SAAP,cAAsB,YAAW;;;;kBAIrC,MACkC,SACH;WAE7B,KAAO,KAAK,sBAAK,4BAAsB,EAAA,MAA8B,GAAI,QAAK,CAAA,CAAA;;;;;OAMhF,MAA0B,SAA+B;WACvD,KAAO,KAAK,iBAAK,4BAAiB,EAAA,MAA8B,GAAI,QAAK,CAAA,CAAA;;;;;WAM3E,MAAkC,SAA+B;WAC/D,KAAO,KAAK,uBAAK,EAAqB,MAAI,GAAI,QAAK,CAAA;;;AAoIvD,0BAAiBC,SAAM;AAAA,GAAA,WAAA,SAAA,CAAA,EAAA;;;AC1JjB,IAAO,SAAP,cAAsB,YAAW;;;;;WAKrC,OAAsB,SAA+B;WACnD,KAAO,IAAK,WAAI,KAAW,IAAK,OAAI;;;;;;OAOtC,SAAkC;WAChC,KAAO,WAAK,WAAW,YAAW,OAAY;;;;;MAMhD,OAAiB,SAA+B;WAC9C,KAAO,OAAK,WAAO,KAAW,IAAK,OAAI;;;AAOrC,IAAO,aAAP,cAA0B,KAAW;AAAA;AAAG,0BAAAC,SAAA;AAAA,GAAA,WAAA,SAAA,CAAA,EAAA;;;AC7BxC,IAAO,cAAP,cAA2B,YAAW;;;;SAI1C,MAC8B,SACC;WAE7B,KAAO,KAAK,gBAAK,EAAc,MAAI,GAAI,QAAK,CAAA;;;AAqMhD,0BAAiBC,cAAW;AAAA,GAAA,gBAAA,cAAA,CAAA,EAAA;;;;ACpItB,IAAO,SAAP,cAA2B,UAAS;;;;;;;;;;;;;;;cAoBxC,IAIqB;;;eACT,QAAK,gBAAW;sBACd,KACR,QAAA,eAAA,OAAA,QAAA,OAAA,SAAA,KAAA;SAEH;QAED,OAAM,SAAyB,CAAA,IAAA;mBACvB,QAAA;YACN,IAAA;;;;UAKF,UAAY;;;eAMN;;;iBAGJ,2BAA4B,mBAAA,GAAA;YAC5B,IAAA;;;;UAUJ;eACA,QAAmB;gBACnB,KAAA,QAA6B,aAAQ,QAAgB,OAAC,SAAA,KAAA;iBACtD,QAAuB;kBACvB,QAAyB;aACzB,QAAmB;;SAEnB,cAAqB,IAAQC,aAAa,IAAA;SAC1C,OAAA,IAAS,KAAsB,IAAI;SAfjC,QAAK,IAAW,MAAO,IAAC;SAExB,aAAc,IAAO,WAAA,IAAA;SACrB,QAAK,IAAY,MAAG,IAAA;AACtB,SAAC,SAAA,IAAA,OAAA,IAAA;AAakB,SAAA,QAAY,IAAA,MAAA,IAAA;SAC7B,cAAY,IAAS,YAAa,IAAA;AACpC,SAAC,SAAA,IAAA,OAAA,IAAA;AAEkB,SAAA,YAAc,IAAA,UAAA,IAAA;SAC/B,WAAO;kBACF;wBACH;;iBAEA;AACJ,WAAC,KAAA,SAAA;;mBAGU;AACX,WAAC;;;MAEM,GAAA,KAAM,SAAQ;IAEd;EACA;EACA,cAAA;AACA,WAAA,EAAA,eAAoB,UAAO,KAAA,MAAA,GAAkB;EAC7C;AACA;AACA,KAAA;AACA,OAAA,SAAA;AACA,OAAA,WAAA;AACA,OAAA,qBAA4B;AAC5B,OAAA,4BAA+B;AAC/B,OAAA,oBAA2B;AAGpC,OAAO,gBAEL;AAaF,OAAM,gBAAyB;AAC/B,OAAM,iBAAuB;AAE7B,OAAA,kBAAuB;OACrB,sBAAmB;OACL,sBAAwB;OACxB,wBAAuB;OAEvB,2BAAuB;AAGvB,IAAA;;;;;;;;;;;;;AAsDhB,IA9DiB;AAgEjB,IAAAC,UAAsB;;;;;;;;;;;;;;;;;;;;;",
  "names": ["_a", "File", "_a", "opts", "Page", "_a", "Page", "_a", "Transcriptions", "Translations", "Audio", "_a", "Chat", "Completions", "_a", "Embeddings", "Edits", "_a", "Images", "Models", "Moderations", "Completions", "toFile"]
}
